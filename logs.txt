* 
* ==> Audit <==
* |--------------|----------------------------------------------|----------|------|---------|---------------------|---------------------|
|   Command    |                     Args                     | Profile  | User | Version |     Start Time      |      End Time       |
|--------------|----------------------------------------------|----------|------|---------|---------------------|---------------------|
| update-check |                                              | minikube | erik | v1.28.0 | 02 May 23 15:42 +04 | 02 May 23 15:42 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 02 May 23 16:29 +04 | 02 May 23 16:29 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 02 May 23 20:12 +04 | 02 May 23 20:12 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 03 May 23 15:12 +04 | 03 May 23 15:12 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 03 May 23 16:07 +04 | 03 May 23 16:07 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 04 May 23 11:38 +04 | 04 May 23 11:38 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 04 May 23 14:35 +04 | 04 May 23 14:35 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 11 May 23 16:54 +04 | 11 May 23 16:54 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 11 May 23 17:57 +04 | 11 May 23 17:57 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 12 May 23 11:51 +04 | 12 May 23 11:51 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 12 May 23 14:56 +04 | 12 May 23 14:56 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 15 May 23 10:52 +04 | 15 May 23 10:52 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 16 May 23 11:41 +04 | 16 May 23 11:41 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 17 May 23 10:53 +04 | 17 May 23 10:53 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 19 May 23 09:37 +04 | 19 May 23 09:37 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 19 May 23 17:08 +04 | 19 May 23 17:08 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 19 May 23 18:35 +04 | 19 May 23 18:35 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 20 May 23 09:26 +04 | 20 May 23 09:26 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 22 May 23 12:19 +04 | 22 May 23 12:19 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 22 May 23 17:03 +04 | 22 May 23 17:03 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 23 May 23 11:22 +04 | 23 May 23 11:23 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 23 May 23 12:32 +04 | 23 May 23 12:32 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 24 May 23 16:13 +04 | 24 May 23 16:13 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 24 May 23 17:26 +04 | 24 May 23 17:27 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 30 May 23 15:33 +04 | 30 May 23 15:33 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 30 May 23 16:16 +04 | 30 May 23 16:16 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 30 May 23 16:59 +04 | 30 May 23 16:59 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 05 Jun 23 15:24 +04 | 05 Jun 23 15:24 +04 |
| start        |                                              | minikube | erik | v1.28.0 | 27 Jun 23 09:57 +04 | 27 Jun 23 10:00 +04 |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 27 Jun 23 11:16 +04 |                     |
| delete       |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:16 +04 | 27 Jun 23 11:16 +04 |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 27 Jun 23 11:16 +04 |                     |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 27 Jun 23 11:17 +04 | 27 Jun 23 11:25 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:21 +04 | 27 Jun 23 11:21 +04 |
| docker-env   |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:29 +04 | 27 Jun 23 11:29 +04 |
| ip           |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:38 +04 | 27 Jun 23 11:38 +04 |
| service      | list                                         | minikube | erik | v1.28.0 | 27 Jun 23 11:40 +04 | 27 Jun 23 11:40 +04 |
| docker-env   |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:42 +04 | 27 Jun 23 11:42 +04 |
| stop         |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:43 +04 | 27 Jun 23 11:43 +04 |
| delete       |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:43 +04 | 27 Jun 23 11:43 +04 |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 27 Jun 23 11:43 +04 | 27 Jun 23 11:46 +04 |
| addons       | enable registry                              | minikube | erik | v1.28.0 | 27 Jun 23 11:47 +04 | 27 Jun 23 11:48 +04 |
| ip           |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:49 +04 | 27 Jun 23 11:49 +04 |
| docker-env   |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:53 +04 | 27 Jun 23 11:53 +04 |
| stop         |                                              | minikube | erik | v1.28.0 | 27 Jun 23 11:53 +04 | 27 Jun 23 11:53 +04 |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 27 Jun 23 11:53 +04 | 27 Jun 23 11:55 +04 |
|              | --insecure-registry=192.168.49.2:5000        |          |      |         |                     |                     |
|              | --docker-env=DOCKER_OPTS=--insecure-registry |          |      |         |                     |                     |
|              | 192.168.49.2:5000                            |          |      |         |                     |                     |
| ip           |                                              | minikube | erik | v1.28.0 | 27 Jun 23 12:00 +04 |                     |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 27 Jun 23 12:00 +04 | 27 Jun 23 12:02 +04 |
|              | --insecure-registry=192.168.49.2:5000        |          |      |         |                     |                     |
|              | --docker-env=DOCKER_OPTS=--insecure-registry |          |      |         |                     |                     |
|              | 192.168.49.2:5000                            |          |      |         |                     |                     |
| stop         |                                              | minikube | erik | v1.28.0 | 27 Jun 23 12:03 +04 | 27 Jun 23 12:03 +04 |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 27 Jun 23 12:03 +04 | 27 Jun 23 12:05 +04 |
|              | --insecure-registry=192.168.49.2:5000        |          |      |         |                     |                     |
| update-check |                                              | minikube | erik | v1.28.0 | 27 Jun 23 16:05 +04 | 27 Jun 23 16:05 +04 |
| update-check |                                              | minikube | erik | v1.28.0 | 27 Jun 23 16:12 +04 | 27 Jun 23 16:12 +04 |
| stop         |                                              | minikube | erik | v1.28.0 | 28 Jun 23 08:42 +04 | 28 Jun 23 08:43 +04 |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 28 Jun 23 08:51 +04 | 28 Jun 23 08:54 +04 |
|              | --insecure-registry=192.168.49.2:5000        |          |      |         |                     |                     |
| docker-env   |                                              | minikube | erik | v1.28.0 | 28 Jun 23 09:00 +04 | 28 Jun 23 09:00 +04 |
| delete       |                                              | minikube | erik | v1.28.0 | 28 Jun 23 09:10 +04 | 28 Jun 23 09:10 +04 |
| start        | --driver=docker                              | minikube | erik | v1.28.0 | 28 Jun 23 09:20 +04 | 28 Jun 23 09:23 +04 |
|              | --insecure-registry=192.168.49.2:5000        |          |      |         |                     |                     |
| docker-env   |                                              | minikube | erik | v1.28.0 | 28 Jun 23 09:28 +04 | 28 Jun 23 09:28 +04 |
| addons       | enable registry                              | minikube | erik | v1.28.0 | 28 Jun 23 09:30 +04 | 28 Jun 23 09:31 +04 |
| addons       | enable ingress                               | minikube | erik | v1.28.0 | 28 Jun 23 09:39 +04 |                     |
|--------------|----------------------------------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/06/28 09:20:48
Running on machine: erik-dell-system-xps-l502x
Binary: Built with gc go1.19.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0628 09:20:48.546864  521800 out.go:296] Setting OutFile to fd 1 ...
I0628 09:20:48.547117  521800 out.go:348] isatty.IsTerminal(1) = true
I0628 09:20:48.547123  521800 out.go:309] Setting ErrFile to fd 2...
I0628 09:20:48.547133  521800 out.go:348] isatty.IsTerminal(2) = true
I0628 09:20:48.547299  521800 root.go:334] Updating PATH: /home/erik/.minikube/bin
I0628 09:20:48.547330  521800 oci.go:572] shell is pointing to dockerd inside minikube. will unset to use host
I0628 09:20:48.548103  521800 out.go:303] Setting JSON to false
I0628 09:20:48.549787  521800 start.go:116] hostinfo: {"hostname":"erik-dell-system-xps-l502x","uptime":246779,"bootTime":1687682869,"procs":309,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.19.0-38-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"63a68b54-c62d-4b7e-bfe9-d8fc7f2579ac"}
I0628 09:20:48.549870  521800 start.go:126] virtualization: kvm host
I0628 09:20:48.611433  521800 out.go:177] üòÑ  minikube v1.28.0 on Ubuntu 22.04
I0628 09:20:48.669402  521800 notify.go:220] Checking for updates...
I0628 09:20:48.710567  521800 out.go:177]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I0628 09:20:48.752533  521800 driver.go:365] Setting default libvirt URI to qemu:///system
I0628 09:20:48.859994  521800 docker.go:137] docker version: linux-23.0.2
I0628 09:20:48.860103  521800 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0628 09:20:49.012772  521800 info.go:266] docker info: {ID:L6KD:J3C7:5EKI:MVYR:6PAF:FO4N:MDDL:IABA:BULC:KGIS:6XCP:VU5L Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:32 SystemTime:2023-06-28 09:20:48.993745496 +0400 +04 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.19.0-38-generic OperatingSystem:Ubuntu 22.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8223629312 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:erik-dell-system-xps-l502x Labels:[] ExperimentalBuild:false ServerVersion:23.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID:v1.1.5-0-gf19387a Expected:v1.1.5-0-gf19387a} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.2] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0628 09:20:49.012883  521800 docker.go:254] overlay module found
I0628 09:20:49.106274  521800 out.go:177] ‚ú®  Using the docker driver based on user configuration
I0628 09:20:49.139285  521800 start.go:282] selected driver: docker
I0628 09:20:49.139344  521800 start.go:808] validating driver "docker" against <nil>
I0628 09:20:49.139397  521800 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0628 09:20:49.139735  521800 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0628 09:20:49.312181  521800 info.go:266] docker info: {ID:L6KD:J3C7:5EKI:MVYR:6PAF:FO4N:MDDL:IABA:BULC:KGIS:6XCP:VU5L Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:32 SystemTime:2023-06-28 09:20:49.301745142 +0400 +04 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.19.0-38-generic OperatingSystem:Ubuntu 22.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8223629312 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:erik-dell-system-xps-l502x Labels:[] ExperimentalBuild:false ServerVersion:23.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID:v1.1.5-0-gf19387a Expected:v1.1.5-0-gf19387a} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.2] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0628 09:20:49.312372  521800 start_flags.go:303] no existing cluster config was found, will generate one from the flags 
I0628 09:20:49.313572  521800 start_flags.go:384] Using suggested 2200MB memory alloc based on sys=7842MB, container=7842MB
I0628 09:20:49.313711  521800 start_flags.go:883] Wait components to verify : map[apiserver:true system_pods:true]
I0628 09:20:49.367380  521800 out.go:177] üìå  Using Docker driver with root privileges
I0628 09:20:49.460893  521800 cni.go:95] Creating CNI manager for ""
I0628 09:20:49.460982  521800 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0628 09:20:49.461021  521800 start_flags.go:317] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[192.168.49.2:5000] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/erik:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0628 09:20:49.573379  521800 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0628 09:20:49.677229  521800 cache.go:120] Beginning downloading kic base image for docker with docker
I0628 09:20:49.751509  521800 out.go:177] üöú  Pulling base image ...
I0628 09:20:49.809341  521800 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I0628 09:20:49.809454  521800 image.go:76] Checking for gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon
I0628 09:20:49.809534  521800 preload.go:148] Found local preload: /home/erik/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4
I0628 09:20:49.809556  521800 cache.go:57] Caching tarball of preloaded images
I0628 09:20:49.810183  521800 preload.go:174] Found /home/erik/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0628 09:20:49.810220  521800 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.3 on docker
I0628 09:20:49.811399  521800 profile.go:148] Saving config to /home/erik/.minikube/profiles/minikube/config.json ...
I0628 09:20:49.811459  521800 lock.go:35] WriteFile acquiring /home/erik/.minikube/profiles/minikube/config.json: {Name:mkb77f8b41ed757d9feb680c5c3f222480edd827 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:20:49.919471  521800 image.go:80] Found gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon, skipping pull
I0628 09:20:49.919495  521800 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 exists in daemon, skipping load
I0628 09:20:49.919510  521800 cache.go:208] Successfully downloaded all kic artifacts
I0628 09:20:49.919552  521800 start.go:364] acquiring machines lock for minikube: {Name:mkdd19898131ba1b96fdd77d0c571c1cb403b23d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0628 09:20:49.919648  521800 start.go:368] acquired machines lock for "minikube" in 80.525¬µs
I0628 09:20:49.919665  521800 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[192.168.49.2:5000] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/erik:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet} &{Name: IP: Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0628 09:20:49.919737  521800 start.go:125] createHost starting for "" (driver="docker")
I0628 09:20:49.999456  521800 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0628 09:20:50.000552  521800 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0628 09:20:50.000633  521800 client.go:168] LocalClient.Create starting
I0628 09:20:50.000954  521800 main.go:134] libmachine: Reading certificate data from /home/erik/.minikube/certs/ca.pem
I0628 09:20:50.001083  521800 main.go:134] libmachine: Decoding PEM data...
I0628 09:20:50.001149  521800 main.go:134] libmachine: Parsing certificate...
I0628 09:20:50.001349  521800 main.go:134] libmachine: Reading certificate data from /home/erik/.minikube/certs/cert.pem
I0628 09:20:50.001427  521800 main.go:134] libmachine: Decoding PEM data...
I0628 09:20:50.001483  521800 main.go:134] libmachine: Parsing certificate...
I0628 09:20:50.002851  521800 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0628 09:20:50.104068  521800 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0628 09:20:50.104135  521800 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I0628 09:20:50.104150  521800 cli_runner.go:164] Run: docker network inspect minikube
W0628 09:20:50.179978  521800 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0628 09:20:50.180005  521800 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0628 09:20:50.180017  521800 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0628 09:20:50.180120  521800 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0628 09:20:50.259404  521800 network.go:295] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc000a2b580] misses:0}
I0628 09:20:50.268567  521800 network.go:241] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I0628 09:20:50.268629  521800 network_create.go:115] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0628 09:20:50.268726  521800 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0628 09:20:51.119574  521800 network_create.go:99] docker network minikube 192.168.49.0/24 created
I0628 09:20:51.119595  521800 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I0628 09:20:51.119674  521800 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0628 09:20:51.201757  521800 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0628 09:20:51.315375  521800 oci.go:103] Successfully created a docker volume minikube
I0628 09:20:51.315440  521800 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 -d /var/lib
I0628 09:21:01.080999  521800 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 -d /var/lib: (9.76545795s)
I0628 09:21:01.081042  521800 oci.go:107] Successfully prepared a docker volume minikube
I0628 09:21:01.081107  521800 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I0628 09:21:01.081160  521800 kic.go:179] Starting extracting preloaded images to volume ...
I0628 09:21:01.081357  521800 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/erik/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 -I lz4 -xf /preloaded.tar -C /extractDir
I0628 09:21:43.125432  521800 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/erik/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 -I lz4 -xf /preloaded.tar -C /extractDir: (42.04392405s)
I0628 09:21:43.125489  521800 kic.go:188] duration metric: took 42.044325 seconds to extract preloaded images to volume
W0628 09:21:43.125785  521800 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0628 09:21:43.125899  521800 oci.go:240] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0628 09:21:43.126015  521800 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0628 09:21:43.286243  521800 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456
I0628 09:21:46.721591  521800 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456: (3.435266344s)
I0628 09:21:46.721709  521800 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0628 09:21:46.803730  521800 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0628 09:21:46.882238  521800 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0628 09:21:47.074951  521800 oci.go:144] the created container "minikube" has a running status.
I0628 09:21:47.074972  521800 kic.go:210] Creating ssh key for kic: /home/erik/.minikube/machines/minikube/id_rsa...
I0628 09:21:47.188600  521800 kic_runner.go:191] docker (temp): /home/erik/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0628 09:21:47.519699  521800 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0628 09:21:47.599960  521800 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0628 09:21:47.599973  521800 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0628 09:21:47.773692  521800 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0628 09:21:47.853526  521800 machine.go:88] provisioning docker machine ...
I0628 09:21:47.853553  521800 ubuntu.go:169] provisioning hostname "minikube"
I0628 09:21:47.853609  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:47.932440  521800 main.go:134] libmachine: Using SSH client type: native
I0628 09:21:47.932640  521800 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0628 09:21:47.932653  521800 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0628 09:21:47.933230  521800 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:34028->127.0.0.1:32787: read: connection reset by peer
I0628 09:21:51.090305  521800 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0628 09:21:51.090377  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:51.203407  521800 main.go:134] libmachine: Using SSH client type: native
I0628 09:21:51.203568  521800 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0628 09:21:51.203588  521800 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0628 09:21:51.325783  521800 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0628 09:21:51.325803  521800 ubuntu.go:175] set auth options {CertDir:/home/erik/.minikube CaCertPath:/home/erik/.minikube/certs/ca.pem CaPrivateKeyPath:/home/erik/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/erik/.minikube/machines/server.pem ServerKeyPath:/home/erik/.minikube/machines/server-key.pem ClientKeyPath:/home/erik/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/erik/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/erik/.minikube}
I0628 09:21:51.325826  521800 ubuntu.go:177] setting up certificates
I0628 09:21:51.325835  521800 provision.go:83] configureAuth start
I0628 09:21:51.325889  521800 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0628 09:21:51.402458  521800 provision.go:138] copyHostCerts
I0628 09:21:51.402510  521800 exec_runner.go:144] found /home/erik/.minikube/ca.pem, removing ...
I0628 09:21:51.402518  521800 exec_runner.go:207] rm: /home/erik/.minikube/ca.pem
I0628 09:21:51.457940  521800 exec_runner.go:151] cp: /home/erik/.minikube/certs/ca.pem --> /home/erik/.minikube/ca.pem (1074 bytes)
I0628 09:21:51.458106  521800 exec_runner.go:144] found /home/erik/.minikube/cert.pem, removing ...
I0628 09:21:51.458113  521800 exec_runner.go:207] rm: /home/erik/.minikube/cert.pem
I0628 09:21:51.458159  521800 exec_runner.go:151] cp: /home/erik/.minikube/certs/cert.pem --> /home/erik/.minikube/cert.pem (1115 bytes)
I0628 09:21:51.458229  521800 exec_runner.go:144] found /home/erik/.minikube/key.pem, removing ...
I0628 09:21:51.458233  521800 exec_runner.go:207] rm: /home/erik/.minikube/key.pem
I0628 09:21:51.458272  521800 exec_runner.go:151] cp: /home/erik/.minikube/certs/key.pem --> /home/erik/.minikube/key.pem (1675 bytes)
I0628 09:21:51.458332  521800 provision.go:112] generating server cert: /home/erik/.minikube/machines/server.pem ca-key=/home/erik/.minikube/certs/ca.pem private-key=/home/erik/.minikube/certs/ca-key.pem org=erik.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0628 09:21:52.045366  521800 provision.go:172] copyRemoteCerts
I0628 09:21:52.045421  521800 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0628 09:21:52.045475  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:52.152665  521800 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/erik/.minikube/machines/minikube/id_rsa Username:docker}
I0628 09:21:52.253631  521800 ssh_runner.go:362] scp /home/erik/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0628 09:21:52.309388  521800 ssh_runner.go:362] scp /home/erik/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0628 09:21:52.332400  521800 ssh_runner.go:362] scp /home/erik/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0628 09:21:52.356717  521800 provision.go:86] duration metric: configureAuth took 1.030868612s
I0628 09:21:52.356734  521800 ubuntu.go:193] setting minikube options for container-runtime
I0628 09:21:52.357187  521800 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I0628 09:21:52.357331  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:52.438522  521800 main.go:134] libmachine: Using SSH client type: native
I0628 09:21:52.438724  521800 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0628 09:21:52.438734  521800 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0628 09:21:52.644849  521800 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0628 09:21:52.644867  521800 ubuntu.go:71] root file system type: overlay
I0628 09:21:52.645211  521800 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0628 09:21:52.645309  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:52.769846  521800 main.go:134] libmachine: Using SSH client type: native
I0628 09:21:52.769999  521800 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0628 09:21:52.770091  521800 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --insecure-registry 192.168.49.2:5000 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0628 09:21:52.918733  521800 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --insecure-registry 192.168.49.2:5000 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0628 09:21:52.918797  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:53.003434  521800 main.go:134] libmachine: Using SSH client type: native
I0628 09:21:53.003598  521800 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0628 09:21:53.003620  521800 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0628 09:21:59.107544  521800 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-10-18 18:18:12.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-06-28 05:21:52.912799626 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --insecure-registry 192.168.49.2:5000 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0628 09:21:59.107564  521800 machine.go:91] provisioned docker machine in 11.254027088s
I0628 09:21:59.107574  521800 client.go:171] LocalClient.Create took 1m9.106933306s
I0628 09:21:59.107597  521800 start.go:167] duration metric: libmachine.API.Create for "minikube" took 1m9.107059035s
I0628 09:21:59.107607  521800 start.go:300] post-start starting for "minikube" (driver="docker")
I0628 09:21:59.107614  521800 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0628 09:21:59.107675  521800 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0628 09:21:59.107743  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:59.193989  521800 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/erik/.minikube/machines/minikube/id_rsa Username:docker}
I0628 09:21:59.310381  521800 ssh_runner.go:195] Run: cat /etc/os-release
I0628 09:21:59.315054  521800 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0628 09:21:59.315072  521800 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0628 09:21:59.315086  521800 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0628 09:21:59.315092  521800 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0628 09:21:59.315102  521800 filesync.go:126] Scanning /home/erik/.minikube/addons for local assets ...
I0628 09:21:59.315174  521800 filesync.go:126] Scanning /home/erik/.minikube/files for local assets ...
I0628 09:21:59.315204  521800 start.go:303] post-start completed in 207.589496ms
I0628 09:21:59.315627  521800 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0628 09:21:59.405482  521800 profile.go:148] Saving config to /home/erik/.minikube/profiles/minikube/config.json ...
I0628 09:21:59.405793  521800 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0628 09:21:59.405840  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:59.484498  521800 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/erik/.minikube/machines/minikube/id_rsa Username:docker}
I0628 09:21:59.591464  521800 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0628 09:21:59.599496  521800 start.go:128] duration metric: createHost completed in 1m9.679738767s
I0628 09:21:59.599515  521800 start.go:83] releasing machines lock for "minikube", held for 1m9.679856538s
I0628 09:21:59.599724  521800 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0628 09:21:59.694209  521800 ssh_runner.go:195] Run: systemctl --version
I0628 09:21:59.694237  521800 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0628 09:21:59.694254  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:59.694285  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:21:59.778512  521800 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/erik/.minikube/machines/minikube/id_rsa Username:docker}
I0628 09:21:59.785551  521800 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/erik/.minikube/machines/minikube/id_rsa Username:docker}
I0628 09:22:02.982377  521800 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (3.288073947s)
I0628 09:22:02.982524  521800 ssh_runner.go:235] Completed: systemctl --version: (3.288288736s)
I0628 09:22:02.982710  521800 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0628 09:22:03.006777  521800 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0628 09:22:03.006971  521800 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0628 09:22:03.042866  521800 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0628 09:22:03.075865  521800 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0628 09:22:03.455119  521800 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0628 09:22:03.606822  521800 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0628 09:22:03.738651  521800 ssh_runner.go:195] Run: sudo systemctl restart docker
I0628 09:22:05.545334  521800 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.806649815s)
I0628 09:22:05.545409  521800 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0628 09:22:05.695015  521800 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0628 09:22:05.828099  521800 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0628 09:22:05.847084  521800 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0628 09:22:05.847152  521800 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0628 09:22:05.851941  521800 start.go:472] Will wait 60s for crictl version
I0628 09:22:05.852007  521800 ssh_runner.go:195] Run: sudo crictl version
I0628 09:22:06.813919  521800 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.20
RuntimeApiVersion:  1.41.0
I0628 09:22:06.813971  521800 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0628 09:22:07.215501  521800 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0628 09:22:07.336562  521800 out.go:204] üê≥  Preparing Kubernetes v1.25.3 on Docker 20.10.20 ...
I0628 09:22:07.336821  521800 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0628 09:22:07.423646  521800 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0628 09:22:07.427201  521800 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0628 09:22:07.517711  521800 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I0628 09:22:07.517776  521800 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0628 09:22:07.616822  521800 docker.go:613] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0628 09:22:07.616838  521800 docker.go:543] Images already preloaded, skipping extraction
I0628 09:22:07.616902  521800 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0628 09:22:07.645903  521800 docker.go:613] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0628 09:22:07.645916  521800 cache_images.go:84] Images are preloaded, skipping loading
I0628 09:22:07.645968  521800 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0628 09:22:10.916190  521800 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (3.27019946s)
I0628 09:22:10.916326  521800 cni.go:95] Creating CNI manager for ""
I0628 09:22:10.916384  521800 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0628 09:22:10.916405  521800 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0628 09:22:10.916450  521800 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I0628 09:22:10.916894  521800 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0628 09:22:10.917169  521800 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0628 09:22:10.917321  521800 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.3
I0628 09:22:11.199923  521800 binaries.go:44] Found k8s binaries, skipping transfer
I0628 09:22:11.200129  521800 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0628 09:22:11.235841  521800 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I0628 09:22:11.280369  521800 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0628 09:22:11.329300  521800 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I0628 09:22:11.394194  521800 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0628 09:22:11.429214  521800 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0628 09:22:11.491025  521800 certs.go:54] Setting up /home/erik/.minikube/profiles/minikube for IP: 192.168.49.2
I0628 09:22:11.491192  521800 certs.go:182] skipping minikubeCA CA generation: /home/erik/.minikube/ca.key
I0628 09:22:11.491274  521800 certs.go:182] skipping proxyClientCA CA generation: /home/erik/.minikube/proxy-client-ca.key
I0628 09:22:11.491338  521800 certs.go:302] generating minikube-user signed cert: /home/erik/.minikube/profiles/minikube/client.key
I0628 09:22:11.491353  521800 crypto.go:68] Generating cert /home/erik/.minikube/profiles/minikube/client.crt with IP's: []
I0628 09:22:11.817446  521800 crypto.go:156] Writing cert to /home/erik/.minikube/profiles/minikube/client.crt ...
I0628 09:22:11.817461  521800 lock.go:35] WriteFile acquiring /home/erik/.minikube/profiles/minikube/client.crt: {Name:mk129a8be0ec29751db2faec3a7d3a5f715757ca Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:22:11.817694  521800 crypto.go:164] Writing key to /home/erik/.minikube/profiles/minikube/client.key ...
I0628 09:22:11.817700  521800 lock.go:35] WriteFile acquiring /home/erik/.minikube/profiles/minikube/client.key: {Name:mk9b8531c02c4451ea27972fb90a9d3144f65daf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:22:11.817821  521800 certs.go:302] generating minikube signed cert: /home/erik/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0628 09:22:11.817834  521800 crypto.go:68] Generating cert /home/erik/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0628 09:22:12.224488  521800 crypto.go:156] Writing cert to /home/erik/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I0628 09:22:12.224504  521800 lock.go:35] WriteFile acquiring /home/erik/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk46b777220344dc210d3928b205f0b6d82a0c80 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:22:12.224713  521800 crypto.go:164] Writing key to /home/erik/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I0628 09:22:12.224720  521800 lock.go:35] WriteFile acquiring /home/erik/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk65337db0faf0f65b3a3e8b27875358f412e022 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:22:12.224822  521800 certs.go:320] copying /home/erik/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /home/erik/.minikube/profiles/minikube/apiserver.crt
I0628 09:22:12.224898  521800 certs.go:324] copying /home/erik/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /home/erik/.minikube/profiles/minikube/apiserver.key
I0628 09:22:12.224962  521800 certs.go:302] generating aggregator signed cert: /home/erik/.minikube/profiles/minikube/proxy-client.key
I0628 09:22:12.224974  521800 crypto.go:68] Generating cert /home/erik/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0628 09:22:12.433623  521800 crypto.go:156] Writing cert to /home/erik/.minikube/profiles/minikube/proxy-client.crt ...
I0628 09:22:12.433638  521800 lock.go:35] WriteFile acquiring /home/erik/.minikube/profiles/minikube/proxy-client.crt: {Name:mk28d6bdd0993634dfc4c38acc07eb52a52a7735 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:22:12.433816  521800 crypto.go:164] Writing key to /home/erik/.minikube/profiles/minikube/proxy-client.key ...
I0628 09:22:12.433822  521800 lock.go:35] WriteFile acquiring /home/erik/.minikube/profiles/minikube/proxy-client.key: {Name:mkc37b49a2252764d6803c8de270bd13e0ed514b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:22:12.434044  521800 certs.go:388] found cert: /home/erik/.minikube/certs/home/erik/.minikube/certs/ca-key.pem (1679 bytes)
I0628 09:22:12.434083  521800 certs.go:388] found cert: /home/erik/.minikube/certs/home/erik/.minikube/certs/ca.pem (1074 bytes)
I0628 09:22:12.434110  521800 certs.go:388] found cert: /home/erik/.minikube/certs/home/erik/.minikube/certs/cert.pem (1115 bytes)
I0628 09:22:12.434136  521800 certs.go:388] found cert: /home/erik/.minikube/certs/home/erik/.minikube/certs/key.pem (1675 bytes)
I0628 09:22:12.434706  521800 ssh_runner.go:362] scp /home/erik/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0628 09:22:12.485313  521800 ssh_runner.go:362] scp /home/erik/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0628 09:22:12.512288  521800 ssh_runner.go:362] scp /home/erik/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0628 09:22:12.538210  521800 ssh_runner.go:362] scp /home/erik/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0628 09:22:12.564358  521800 ssh_runner.go:362] scp /home/erik/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0628 09:22:12.593595  521800 ssh_runner.go:362] scp /home/erik/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0628 09:22:12.619364  521800 ssh_runner.go:362] scp /home/erik/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0628 09:22:12.645548  521800 ssh_runner.go:362] scp /home/erik/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0628 09:22:12.671006  521800 ssh_runner.go:362] scp /home/erik/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0628 09:22:12.698066  521800 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0628 09:22:12.716474  521800 ssh_runner.go:195] Run: openssl version
I0628 09:22:12.724065  521800 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0628 09:22:12.749273  521800 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0628 09:22:12.754185  521800 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Jan 18 14:41 /usr/share/ca-certificates/minikubeCA.pem
I0628 09:22:12.754230  521800 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0628 09:22:12.761285  521800 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0628 09:22:12.772117  521800 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[192.168.49.2:5000] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/erik:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0628 09:22:12.772265  521800 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0628 09:22:12.832173  521800 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0628 09:22:12.840908  521800 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0628 09:22:12.851513  521800 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I0628 09:22:12.851579  521800 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0628 09:22:12.870578  521800 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0628 09:22:12.870621  521800 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0628 09:22:12.910545  521800 kubeadm.go:317] W0628 05:22:12.909802    1017 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I0628 09:22:13.570252  521800 kubeadm.go:317] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0628 09:22:14.130201  521800 kubeadm.go:317] 	[WARNING SystemVerification]: missing optional cgroups: blkio
I0628 09:22:14.130985  521800 kubeadm.go:317] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/5.19.0-38-generic\n", err: exit status 1
I0628 09:22:14.257000  521800 kubeadm.go:317] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0628 09:23:03.895999  521800 kubeadm.go:317] [init] Using Kubernetes version: v1.25.3
I0628 09:23:03.901511  521800 kubeadm.go:317] [preflight] Running pre-flight checks
I0628 09:23:03.902045  521800 kubeadm.go:317] [preflight] The system verification failed. Printing the output from the verification:
I0628 09:23:03.902371  521800 kubeadm.go:317] [0;37mKERNEL_VERSION[0m: [0;32m5.19.0-38-generic[0m
I0628 09:23:03.902589  521800 kubeadm.go:317] [0;37mOS[0m: [0;32mLinux[0m
I0628 09:23:03.902805  521800 kubeadm.go:317] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0628 09:23:03.903020  521800 kubeadm.go:317] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0628 09:23:03.903299  521800 kubeadm.go:317] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0628 09:23:03.903577  521800 kubeadm.go:317] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0628 09:23:03.903773  521800 kubeadm.go:317] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0628 09:23:03.903986  521800 kubeadm.go:317] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0628 09:23:03.904371  521800 kubeadm.go:317] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0628 09:23:03.904595  521800 kubeadm.go:317] [0;37mCGROUPS_BLKIO[0m: [0;33mmissing[0m
I0628 09:23:03.904902  521800 kubeadm.go:317] [preflight] Pulling images required for setting up a Kubernetes cluster
I0628 09:23:03.905289  521800 kubeadm.go:317] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0628 09:23:03.905681  521800 kubeadm.go:317] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0628 09:23:03.905980  521800 kubeadm.go:317] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0628 09:23:04.321628  521800 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0628 09:23:04.379798  521800 kubeadm.go:317] [certs] Using existing ca certificate authority
I0628 09:23:04.380213  521800 kubeadm.go:317] [certs] Using existing apiserver certificate and key on disk
I0628 09:23:04.380728  521800 kubeadm.go:317] [certs] Generating "apiserver-kubelet-client" certificate and key
I0628 09:23:04.380977  521800 kubeadm.go:317] [certs] Generating "front-proxy-ca" certificate and key
I0628 09:23:04.381225  521800 kubeadm.go:317] [certs] Generating "front-proxy-client" certificate and key
I0628 09:23:04.381424  521800 kubeadm.go:317] [certs] Generating "etcd/ca" certificate and key
I0628 09:23:04.381623  521800 kubeadm.go:317] [certs] Generating "etcd/server" certificate and key
I0628 09:23:04.382234  521800 kubeadm.go:317] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0628 09:23:04.382666  521800 kubeadm.go:317] [certs] Generating "etcd/peer" certificate and key
I0628 09:23:04.383404  521800 kubeadm.go:317] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0628 09:23:04.383763  521800 kubeadm.go:317] [certs] Generating "etcd/healthcheck-client" certificate and key
I0628 09:23:04.384082  521800 kubeadm.go:317] [certs] Generating "apiserver-etcd-client" certificate and key
I0628 09:23:04.384410  521800 kubeadm.go:317] [certs] Generating "sa" key and public key
I0628 09:23:04.384775  521800 kubeadm.go:317] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0628 09:23:04.385116  521800 kubeadm.go:317] [kubeconfig] Writing "admin.conf" kubeconfig file
I0628 09:23:04.385440  521800 kubeadm.go:317] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0628 09:23:04.385731  521800 kubeadm.go:317] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0628 09:23:04.386475  521800 kubeadm.go:317] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0628 09:23:04.387295  521800 kubeadm.go:317] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0628 09:23:04.387999  521800 kubeadm.go:317] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0628 09:23:04.388358  521800 kubeadm.go:317] [kubelet-start] Starting the kubelet
I0628 09:23:04.388774  521800 kubeadm.go:317] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0628 09:23:04.556996  521800 out.go:204]     ‚ñ™ Booting up control plane ...
I0628 09:23:04.557969  521800 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0628 09:23:04.558593  521800 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0628 09:23:04.559147  521800 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0628 09:23:04.559790  521800 kubeadm.go:317] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0628 09:23:04.560927  521800 kubeadm.go:317] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0628 09:23:04.561503  521800 kubeadm.go:317] [apiclient] All control plane components are healthy after 36.522559 seconds
I0628 09:23:04.562212  521800 kubeadm.go:317] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0628 09:23:04.563220  521800 kubeadm.go:317] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0628 09:23:04.563737  521800 kubeadm.go:317] [upload-certs] Skipping phase. Please see --upload-certs
I0628 09:23:04.565105  521800 kubeadm.go:317] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0628 09:23:04.565447  521800 kubeadm.go:317] [bootstrap-token] Using token: rgi283.rqkv7mderdymup12
I0628 09:23:04.650637  521800 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0628 09:23:04.651541  521800 kubeadm.go:317] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0628 09:23:04.652205  521800 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0628 09:23:04.653016  521800 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0628 09:23:04.653761  521800 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0628 09:23:04.654431  521800 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0628 09:23:04.655026  521800 kubeadm.go:317] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0628 09:23:04.655627  521800 kubeadm.go:317] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0628 09:23:04.655861  521800 kubeadm.go:317] [kubelet-check] Initial timeout of 40s passed.
I0628 09:23:04.656135  521800 kubeadm.go:317] [addons] Applied essential addon: CoreDNS
I0628 09:23:04.656367  521800 kubeadm.go:317] [addons] Applied essential addon: kube-proxy
I0628 09:23:04.656421  521800 kubeadm.go:317] 
I0628 09:23:04.656724  521800 kubeadm.go:317] Your Kubernetes control-plane has initialized successfully!
I0628 09:23:04.656745  521800 kubeadm.go:317] 
I0628 09:23:04.657108  521800 kubeadm.go:317] To start using your cluster, you need to run the following as a regular user:
I0628 09:23:04.657131  521800 kubeadm.go:317] 
I0628 09:23:04.657235  521800 kubeadm.go:317]   mkdir -p $HOME/.kube
I0628 09:23:04.657517  521800 kubeadm.go:317]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0628 09:23:04.657795  521800 kubeadm.go:317]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0628 09:23:04.657815  521800 kubeadm.go:317] 
I0628 09:23:04.658069  521800 kubeadm.go:317] Alternatively, if you are the root user, you can run:
I0628 09:23:04.658101  521800 kubeadm.go:317] 
I0628 09:23:04.658310  521800 kubeadm.go:317]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0628 09:23:04.658328  521800 kubeadm.go:317] 
I0628 09:23:04.658581  521800 kubeadm.go:317] You should now deploy a pod network to the cluster.
I0628 09:23:04.658917  521800 kubeadm.go:317] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0628 09:23:04.659236  521800 kubeadm.go:317]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0628 09:23:04.659255  521800 kubeadm.go:317] 
I0628 09:23:04.659598  521800 kubeadm.go:317] You can now join any number of control-plane nodes by copying certificate authorities
I0628 09:23:04.659921  521800 kubeadm.go:317] and service account keys on each node and then running the following as root:
I0628 09:23:04.659951  521800 kubeadm.go:317] 
I0628 09:23:04.660403  521800 kubeadm.go:317]   kubeadm join control-plane.minikube.internal:8443 --token rgi283.rqkv7mderdymup12 \
I0628 09:23:04.660838  521800 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:be0c65affcf279b18b774a789427ef3cc2c4c05f12d3b1cd4d09bbe5dc7dca29 \
I0628 09:23:04.660933  521800 kubeadm.go:317] 	--control-plane 
I0628 09:23:04.660949  521800 kubeadm.go:317] 
I0628 09:23:04.661339  521800 kubeadm.go:317] Then you can join any number of worker nodes by running the following on each as root:
I0628 09:23:04.661368  521800 kubeadm.go:317] 
I0628 09:23:04.661711  521800 kubeadm.go:317] kubeadm join control-plane.minikube.internal:8443 --token rgi283.rqkv7mderdymup12 \
I0628 09:23:04.662158  521800 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:be0c65affcf279b18b774a789427ef3cc2c4c05f12d3b1cd4d09bbe5dc7dca29 
I0628 09:23:04.662192  521800 cni.go:95] Creating CNI manager for ""
I0628 09:23:04.662215  521800 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0628 09:23:04.662299  521800 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0628 09:23:04.662498  521800 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.3/kubectl label nodes minikube.k8s.io/version=v1.28.0 minikube.k8s.io/commit=986b1ebd987211ed16f8cc10aed7d2c42fc8392f minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_06_28T09_23_04_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0628 09:23:04.662500  521800 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0628 09:23:05.091146  521800 kubeadm.go:1067] duration metric: took 428.830712ms to wait for elevateKubeSystemPrivileges.
I0628 09:23:05.091231  521800 ops.go:34] apiserver oom_adj: -16
I0628 09:23:05.091267  521800 kubeadm.go:398] StartCluster complete in 52.319162842s
I0628 09:23:05.091314  521800 settings.go:142] acquiring lock: {Name:mk3445869d5673a0b78fb41c5e95f564cfdb46b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:23:05.101483  521800 settings.go:150] Updating kubeconfig:  /home/erik/.kube/config
I0628 09:23:05.150101  521800 lock.go:35] WriteFile acquiring /home/erik/.kube/config: {Name:mkf8c8a69bb38d3a8c5886914a35959c159a2884 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0628 09:23:05.826540  521800 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0628 09:23:05.826667  521800 start.go:212] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0628 09:23:05.826755  521800 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0628 09:23:05.826827  521800 addons.go:486] enableAddons start: toEnable=map[], additional=[]
I0628 09:23:05.877729  521800 out.go:177] üîé  Verifying Kubernetes components...
I0628 09:23:05.827374  521800 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I0628 09:23:05.877906  521800 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0628 09:23:05.918686  521800 addons.go:227] Setting addon storage-provisioner=true in "minikube"
W0628 09:23:05.918705  521800 addons.go:236] addon storage-provisioner should already be in state true
I0628 09:23:05.918725  521800 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0628 09:23:05.877969  521800 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0628 09:23:05.918748  521800 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0628 09:23:05.918799  521800 host.go:66] Checking if "minikube" exists ...
I0628 09:23:05.919141  521800 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0628 09:23:05.919335  521800 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0628 09:23:06.357232  521800 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0628 09:23:06.359352  521800 api_server.go:51] waiting for apiserver process to appear ...
I0628 09:23:06.359392  521800 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0628 09:23:07.238105  521800 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0628 09:23:07.238157  521800 addons.go:227] Setting addon default-storageclass=true in "minikube"
W0628 09:23:07.238214  521800 addons.go:236] addon default-storageclass should already be in state true
I0628 09:23:07.329133  521800 host.go:66] Checking if "minikube" exists ...
I0628 09:23:07.329234  521800 addons.go:419] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0628 09:23:07.329243  521800 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0628 09:23:07.329294  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:23:07.329575  521800 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0628 09:23:07.439689  521800 addons.go:419] installing /etc/kubernetes/addons/storageclass.yaml
I0628 09:23:07.476339  521800 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0628 09:23:07.476417  521800 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0628 09:23:07.525991  521800 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/erik/.minikube/machines/minikube/id_rsa Username:docker}
I0628 09:23:08.130618  521800 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/erik/.minikube/machines/minikube/id_rsa Username:docker}
I0628 09:23:09.817361  521800 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0628 09:23:09.891898  521800 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0628 09:23:10.062520  521800 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (3.705259451s)
I0628 09:23:10.062542  521800 start.go:826] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS
I0628 09:23:10.062557  521800 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.703147272s)
I0628 09:23:10.062578  521800 api_server.go:71] duration metric: took 4.235860072s to wait for apiserver process to appear ...
I0628 09:23:10.062585  521800 api_server.go:87] waiting for apiserver healthz status ...
I0628 09:23:10.078681  521800 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0628 09:23:11.029861  521800 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I0628 09:23:11.031520  521800 api_server.go:140] control plane version: v1.25.3
I0628 09:23:11.031540  521800 api_server.go:130] duration metric: took 968.948233ms to wait for apiserver health ...
I0628 09:23:11.033153  521800 system_pods.go:43] waiting for kube-system pods to appear ...
I0628 09:23:11.120663  521800 system_pods.go:59] 4 kube-system pods found
I0628 09:23:11.120738  521800 system_pods.go:61] "etcd-minikube" [dc1fd3d1-836c-4cb4-aef3-c7e764148f44] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0628 09:23:11.120768  521800 system_pods.go:61] "kube-apiserver-minikube" [7099818e-eddc-4ee4-8f4c-6871e4041a90] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0628 09:23:11.120794  521800 system_pods.go:61] "kube-controller-manager-minikube" [2a62a507-6994-4d86-bd21-ff684643ed4f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0628 09:23:11.120817  521800 system_pods.go:61] "kube-scheduler-minikube" [517cecfd-0f85-4c50-92b6-368378341578] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0628 09:23:11.120832  521800 system_pods.go:74] duration metric: took 87.661723ms to wait for pod list to return data ...
I0628 09:23:11.120855  521800 kubeadm.go:573] duration metric: took 5.294134149s to wait for : map[apiserver:true system_pods:true] ...
I0628 09:23:11.120889  521800 node_conditions.go:102] verifying NodePressure condition ...
I0628 09:23:11.682345  521800 node_conditions.go:122] node storage ephemeral capacity is 719433832Ki
I0628 09:23:11.682374  521800 node_conditions.go:123] node cpu capacity is 8
I0628 09:23:11.682392  521800 node_conditions.go:105] duration metric: took 561.494591ms to run NodePressure ...
I0628 09:23:11.682408  521800 start.go:217] waiting for startup goroutines ...
I0628 09:23:12.561318  521800 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.743921716s)
I0628 09:23:12.561339  521800 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.669418136s)
I0628 09:23:12.743139  521800 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0628 09:23:12.957258  521800 addons.go:488] enableAddons completed in 7.130442988s
I0628 09:23:12.958510  521800 ssh_runner.go:195] Run: rm -f paused
I0628 09:23:13.330639  521800 start.go:506] kubectl: 1.26.0, cluster: 1.25.3 (minor skew: 1)
I0628 09:23:13.476464  521800 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Wed 2023-06-28 05:21:48 UTC, end at Wed 2023-06-28 05:46:41 UTC. --
Jun 28 05:22:03 minikube dockerd[385]: time="2023-06-28T05:22:03.752389132Z" level=info msg="Processing signal 'terminated'"
Jun 28 05:22:03 minikube dockerd[385]: time="2023-06-28T05:22:03.777529845Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jun 28 05:22:03 minikube dockerd[385]: time="2023-06-28T05:22:03.780193788Z" level=info msg="Daemon shutdown complete"
Jun 28 05:22:03 minikube systemd[1]: docker.service: Succeeded.
Jun 28 05:22:03 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 28 05:22:03 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.855487095Z" level=info msg="Starting up"
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.857794029Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.857813638Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.857838109Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.857852631Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.859275388Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.859295977Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.859313555Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Jun 28 05:22:03 minikube dockerd[592]: time="2023-06-28T05:22:03.859326381Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Jun 28 05:22:04 minikube dockerd[592]: time="2023-06-28T05:22:04.152369432Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Jun 28 05:22:04 minikube dockerd[592]: time="2023-06-28T05:22:04.343010762Z" level=info msg="Loading containers: start."
Jun 28 05:22:05 minikube dockerd[592]: time="2023-06-28T05:22:05.117393145Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jun 28 05:22:05 minikube dockerd[592]: time="2023-06-28T05:22:05.248535073Z" level=info msg="Loading containers: done."
Jun 28 05:22:05 minikube dockerd[592]: time="2023-06-28T05:22:05.402126048Z" level=info msg="Docker daemon" commit=03df974 graphdriver(s)=overlay2 version=20.10.20
Jun 28 05:22:05 minikube dockerd[592]: time="2023-06-28T05:22:05.402303738Z" level=info msg="Daemon has completed initialization"
Jun 28 05:22:05 minikube systemd[1]: Started Docker Application Container Engine.
Jun 28 05:22:05 minikube dockerd[592]: time="2023-06-28T05:22:05.556245655Z" level=info msg="API listen on [::]:2376"
Jun 28 05:22:05 minikube dockerd[592]: time="2023-06-28T05:22:05.561348601Z" level=info msg="API listen on /var/run/docker.sock"
Jun 28 05:23:54 minikube dockerd[592]: time="2023-06-28T05:23:54.220699609Z" level=info msg="ignoring event" container=e49f7a10e3d5f59008ed26bf3dd7be23313357b92cf82baa03c2074f09e0fdf4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.205338641Z" level=info msg="parsed scheme: \"\"" module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.210616431Z" level=info msg="scheme \"\" not registered, fallback to default scheme" module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.210641909Z" level=info msg="ccResolverWrapper: sending update to cc: {[{localhost  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.210656184Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.326677563Z" level=warning msg="grpc: addrConn.createTransport failed to connect to {localhost  <nil> 0 <nil>}. Err :connection error: desc = \"transport: Error while dialing only one connection allowed\". Reconnecting..." module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.866054257Z" level=info msg="parsed scheme: \"\"" module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.866110786Z" level=info msg="scheme \"\" not registered, fallback to default scheme" module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.866132835Z" level=info msg="ccResolverWrapper: sending update to cc: {[{localhost  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Jun 28 05:24:12 minikube dockerd[592]: time="2023-06-28T05:24:12.866146412Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Jun 28 05:25:44 minikube dockerd[592]: time="2023-06-28T05:25:44.753800458Z" level=warning msg="grpc: addrConn.createTransport failed to connect to {localhost  <nil> 0 <nil>}. Err :connection error: desc = \"transport: Error while dialing only one connection allowed\". Reconnecting..." module=grpc
Jun 28 05:29:15 minikube dockerd[592]: time="2023-06-28T05:29:15.816364776Z" level=info msg="Attempting next endpoint for push after error: Get \"https://10.108.70.20/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 28 05:29:30 minikube dockerd[592]: time="2023-06-28T05:29:30.817883786Z" level=error msg="Not continuing with push after error: Get \"http://10.108.70.20/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 28 05:30:30 minikube dockerd[592]: time="2023-06-28T05:30:30.626521050Z" level=warning msg="reference for unknown type: " digest="sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da" remote="gcr.io/google_containers/kube-registry-proxy@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da"
Jun 28 05:31:23 minikube dockerd[592]: time="2023-06-28T05:31:23.184832728Z" level=warning msg="reference for unknown type: " digest="sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96" remote="docker.io/library/registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96"
Jun 28 05:38:16 minikube dockerd[592]: time="2023-06-28T05:38:16.514937050Z" level=info msg="Attempting next endpoint for push after error: Get \"https://10.99.75.183/v2/\": dial tcp 10.99.75.183:443: connect: connection refused"
Jun 28 05:40:06 minikube dockerd[592]: time="2023-06-28T05:40:06.366766949Z" level=warning msg="reference for unknown type: " digest="sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660" remote="k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660"
Jun 28 05:40:31 minikube dockerd[592]: time="2023-06-28T05:40:31.201044731Z" level=info msg="ignoring event" container=3924db5e19ed282f2c418b53f1e5c68322b60fae6e0e725a4260d234ee1c730c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 28 05:40:32 minikube dockerd[592]: time="2023-06-28T05:40:32.252148433Z" level=info msg="ignoring event" container=b989d322244f9218ba024afd1cfe59f3cd85256f5e183ed9efd8b2018f484409 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 28 05:40:35 minikube dockerd[592]: time="2023-06-28T05:40:35.950456201Z" level=info msg="ignoring event" container=cdd078f0206d037fe422814f4c4c5884bb218ed95799ef8f33f3215a33d75f6a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 28 05:40:35 minikube dockerd[592]: time="2023-06-28T05:40:35.962818860Z" level=info msg="ignoring event" container=a4dd273c0fa8a8647b677b3e7451fde59992d56130ee6fabc8a9566932c58584 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 28 05:41:05 minikube dockerd[592]: time="2023-06-28T05:41:05.528260319Z" level=warning msg="reference for unknown type: " digest="sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8" remote="k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Jun 28 05:43:04 minikube dockerd[592]: time="2023-06-28T05:43:04.263603900Z" level=error msg="Not continuing with pull after error: context canceled"
Jun 28 05:45:16 minikube dockerd[592]: time="2023-06-28T05:45:16.766760586Z" level=error msg="Not continuing with pull after error: context canceled"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289047477Z" level=info msg="Layer sha256:13efbce9781e9f23c3a583e8d7395cd3dc442c606813aa9e6c5c8762126f8d06 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289079513Z" level=info msg="Layer sha256:44f86b2765aac48b3c83882c0003e5721a7896211b2f6c3f123da61996ee5809 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289090358Z" level=info msg="Layer sha256:86b6447848d21c5d2ba903c99bbc37c79a481be3b3d706c6116dcea8caa83659 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289099900Z" level=info msg="Layer sha256:040861bbdab467e13f24f78eb26a210cf5076336a9bd87d58cac622eb57d9a1f cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289109736Z" level=info msg="Layer sha256:fbf89908007da471d360cc8987324430ffaf1a14ff2336915a30da9389fa13d2 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289121491Z" level=info msg="Layer sha256:c8b69243546f1a430ca011ecae91828f62dd44cf9893c43f6fe20eceb72a99b4 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289130645Z" level=info msg="Layer sha256:7d1d579acfd81bb824fa6043fd93437ea70a3e1fbd977d1acce5c6aa908ba2b9 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289139507Z" level=info msg="Layer sha256:1716a5ef4c023ba53423deec307126d2755a30bf0d751e4fdcca098ef514f8fa cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289148452Z" level=info msg="Layer sha256:5d97ed503fc657e822aa28dfbc5262fb5410da95fcd7625dde6d1e7eb9ad1efe cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289157211Z" level=info msg="Layer sha256:fe57448b144044a5de926edfe4e6f8c349e3dad04692ddaa29903a3f59562da4 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289166484Z" level=info msg="Layer sha256:f3b1b47d52fc92f19c9d7a6288877fcb47dd28d73d353d87656d5cc4ef28dd63 cleaned up"
Jun 28 05:45:17 minikube dockerd[592]: time="2023-06-28T05:45:17.289175475Z" level=info msg="Layer sha256:b541d28bf3b491aeb424c61353c8c92476ecc2cd603a6c09ee5c2708f1a4b258 cleaned up"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID
b989d322244f9       k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660   6 minutes ago       Exited              patch                     0                   cdd078f0206d0
3924db5e19ed2       k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660   6 minutes ago       Exited              create                    0                   a4dd273c0fa8a
28f0564a392dd       registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96                                        15 minutes ago      Running             registry                  0                   d22be533aa92d
a743c7dcba285       gcr.io/google_containers/kube-registry-proxy@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da    15 minutes ago      Running             registry-proxy            0                   9c19141ad91a6
4d1163e9687aa       6e38f40d628db                                                                                                           22 minutes ago      Running             storage-provisioner       1                   686b690eb22d1
5cb29a0fa2e38       5185b96f0becf                                                                                                           23 minutes ago      Running             coredns                   0                   261ef2494461b
e49f7a10e3d5f       6e38f40d628db                                                                                                           23 minutes ago      Exited              storage-provisioner       0                   686b690eb22d1
7d7fba0770376       beaaf00edd38a                                                                                                           23 minutes ago      Running             kube-proxy                0                   a2eb0afe8f5d2
50a24ab50621b       a8a176a5d5d69                                                                                                           24 minutes ago      Running             etcd                      0                   7fcf5a86a23cb
1028df588db3e       6d23ec0e8b87e                                                                                                           24 minutes ago      Running             kube-scheduler            0                   76c06bb18bd35
2ffce95599f12       6039992312758                                                                                                           24 minutes ago      Running             kube-controller-manager   0                   2606999d691c4
e4d7d3011faa0       0346dbd74bcb9                                                                                                           24 minutes ago      Running             kube-apiserver            0                   3385f781bc79e

* 
* ==> coredns [5cb29a0fa2e3] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=986b1ebd987211ed16f8cc10aed7d2c42fc8392f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_06_28T09_23_04_0700
                    minikube.k8s.io/version=v1.28.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 28 Jun 2023 05:22:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 28 Jun 2023 05:46:35 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 28 Jun 2023 05:45:45 +0000   Wed, 28 Jun 2023 05:22:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 28 Jun 2023 05:45:45 +0000   Wed, 28 Jun 2023 05:22:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 28 Jun 2023 05:45:45 +0000   Wed, 28 Jun 2023 05:22:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 28 Jun 2023 05:45:45 +0000   Wed, 28 Jun 2023 05:23:13 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  719433832Ki
  hugepages-2Mi:      0
  memory:             8030888Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  719433832Ki
  hugepages-2Mi:      0
  memory:             8030888Ki
  pods:               110
System Info:
  Machine ID:                 996614ec4c814b87b7ec8ebee3d0e8c9
  System UUID:                7db62567-aeb5-4963-b88c-e0a657259769
  Boot ID:                    8d52f9a9-4f10-4f8f-aa36-4512d73e3d3c
  Kernel Version:             5.19.0-38-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.20
  Kubelet Version:            v1.25.3
  Kube-Proxy Version:         v1.25.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-5959f988fd-6dnfw    100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         6m46s
  kube-system                 coredns-565d847f94-nfvr5                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     23m
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         23m
  kube-system                 kube-apiserver-minikube                      250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 kube-controller-manager-minikube             200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 kube-proxy-ppxfb                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 registry-lfx4q                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
  kube-system                 registry-proxy-k2bct                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (3%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 23m   kube-proxy       
  Normal  Starting                 23m   kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  23m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    23m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     23m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  23m   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeNotReady             23m   kubelet          Node minikube status is now: NodeNotReady
  Normal  RegisteredNode           23m   node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                23m   kubelet          Node minikube status is now: NodeReady

* 
* ==> dmesg <==
* [  +0.000037] iwlwifi 0000:03:00.0: FW error in SYNC CMD REPLY_ADD_STA
[  +0.000004] CPU: 7 PID: 6804 Comm: kworker/u16:59 Tainted: G           OE     5.19.0-38-generic #39~22.04.1-Ubuntu
[  +0.000003] Hardware name: Dell Inc.          Dell System XPS L502X/0YR8NN, BIOS A12 09/07/2012
[  +0.000001] Workqueue: phy1 ieee80211_iface_work [mac80211]
[  +0.000055] Call Trace:
[  +0.000002]  <TASK>
[  +0.000003]  show_stack+0x52/0x69
[  +0.000006]  dump_stack_lvl+0x49/0x6d
[  +0.000006]  dump_stack+0x10/0x18
[  +0.000003]  iwl_trans_txq_send_hcmd_sync+0x373/0x380 [iwlwifi]
[  +0.000024]  ? destroy_sched_domains_rcu+0x40/0x40
[  +0.000005]  iwl_trans_txq_send_hcmd+0xbd/0x170 [iwlwifi]
[  +0.000019]  iwl_trans_send_cmd+0x84/0x130 [iwlwifi]
[  +0.000018]  iwl_dvm_send_cmd+0x54/0xe0 [iwldvm]
[  +0.000010]  iwl_send_add_sta+0xae/0x370 [iwldvm]
[  +0.000011]  iwl_sta_rx_agg_stop+0x17a/0x2b0 [iwldvm]
[  +0.000009]  ? iwl_sta_rx_agg_stop+0x17a/0x2b0 [iwldvm]
[  +0.000010]  iwlagn_mac_ampdu_action+0x25b/0x350 [iwldvm]
[  +0.000008]  drv_ampdu_action+0x6f/0x1a0 [mac80211]
[  +0.000033]  ___ieee80211_stop_rx_ba_session+0x103/0x2a0 [mac80211]
[  +0.000036]  ieee80211_sta_tear_down_BA_sessions+0x4e/0x130 [mac80211]
[  +0.000035]  __sta_info_destroy_part1+0x3e/0x1f0 [mac80211]
[  +0.000032]  __sta_info_flush+0xd0/0x190 [mac80211]
[  +0.000033]  ieee80211_set_disassoc+0x158/0x590 [mac80211]
[  +0.000046]  ieee80211_sta_connection_lost+0x3c/0xc0 [mac80211]
[  +0.000045]  ieee80211_sta_work+0x60a/0x6e0 [mac80211]
[  +0.000045]  ieee80211_iface_work+0x15d/0x1a0 [mac80211]
[  +0.000036]  process_one_work+0x21f/0x400
[  +0.000003]  worker_thread+0x50/0x3f0
[  +0.000002]  ? rescuer_thread+0x3a0/0x3a0
[  +0.000002]  kthread+0xee/0x120
[  +0.000003]  ? kthread_complete_and_exit+0x20/0x20
[  +0.000003]  ret_from_fork+0x22/0x30
[  +0.000005]  </TASK>
[  +0.017214] iwlwifi 0000:03:00.0: Fw not loaded - dropping CMD: 18
[  +0.000035] iwlwifi 0000:03:00.0: Fw not loaded - dropping CMD: 18
[  +0.000013] iwlwifi 0000:03:00.0: Fw not loaded - dropping CMD: 18
[  +0.000013] iwlwifi 0000:03:00.0: Fw not loaded - dropping CMD: 1e
[  +0.000006] iwlwifi 0000:03:00.0: Couldn't flush the AGG queue
[  +0.000007] iwlwifi 0000:03:00.0: Fw not loaded - dropping CMD: 1e
[  +0.000005] iwlwifi 0000:03:00.0: Couldn't flush the AGG queue
[  +0.031896] iwlwifi 0000:03:00.0: iwl_trans_wait_tx_queues_empty bad state = 0
[Jun25 11:39] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.221410] mt7601u 3-2:1.0: Error: MCU response pre-completed!
[Jun25 11:41] kauditd_printk_skb: 89 callbacks suppressed
[Jun25 16:55] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.220279] mt7601u 3-2:1.0: Error: MCU response pre-completed!
[Jun25 17:03] i915 0000:00:02.0: [drm] *ERROR* uncleared pch fifo underrun on pch transcoder A
[  +0.000029] i915 0000:00:02.0: [drm] *ERROR* PCH transcoder A FIFO underrun
[Jun25 17:05] SUPR0GipMap: fGetGipCpu=0x1b
[  +0.938589] vboxdrv: 0000000000000000 VMMR0.r0
[  +0.988199] VBoxNetFlt: attached to 'vboxnet0' / 0a:00:27:00:00:00
[  +1.055524] vboxdrv: 0000000000000000 VBoxDDR0.r0
[Jun25 17:45] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.220855] mt7601u 3-2:1.0: Error: MCU response pre-completed!
[Jun25 18:24] vboxnetflt: 334 out of 648 packets were not sent (directed to host)
[Jun26 01:19] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.219774] mt7601u 3-2:1.0: Error: MCU response pre-completed!
[Jun26 01:55] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.225803] mt7601u 3-2:1.0: Error: MCU response pre-completed!

* 
* ==> etcd [50a24ab50621] <==
* {"level":"warn","ts":"2023-06-28T05:45:05.791Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"160.465094ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022062539163800 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1428 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128022062539163798 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2023-06-28T05:45:05.791Z","caller":"traceutil/trace.go:171","msg":"trace[1535392183] linearizableReadLoop","detail":"{readStateIndex:1726; appliedIndex:1725; }","duration":"355.370687ms","start":"2023-06-28T05:45:05.436Z","end":"2023-06-28T05:45:05.791Z","steps":["trace[1535392183] 'read index received'  (duration: 194.36938ms)","trace[1535392183] 'applied index is now lower than readState.Index'  (duration: 160.99607ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:45:05.792Z","caller":"traceutil/trace.go:171","msg":"trace[718113233] transaction","detail":"{read_only:false; response_revision:1435; number_of_response:1; }","duration":"482.185218ms","start":"2023-06-28T05:45:05.309Z","end":"2023-06-28T05:45:05.791Z","steps":["trace[718113233] 'process raft request'  (duration: 321.020519ms)","trace[718113233] 'compare'  (duration: 160.155559ms)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:05.792Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"141.144091ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/\" range_end:\"/registry/resourcequotas0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2023-06-28T05:45:05.792Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-06-28T05:45:05.309Z","time spent":"482.442449ms","remote":"127.0.0.1:53438","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1428 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128022062539163798 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2023-06-28T05:45:05.792Z","caller":"traceutil/trace.go:171","msg":"trace[1570252153] range","detail":"{range_begin:/registry/resourcequotas/; range_end:/registry/resourcequotas0; response_count:0; response_revision:1435; }","duration":"141.299618ms","start":"2023-06-28T05:45:05.651Z","end":"2023-06-28T05:45:05.792Z","steps":["trace[1570252153] 'agreement among raft nodes before linearized reading'  (duration: 140.936986ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:45:05.792Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"356.311485ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:05.792Z","caller":"traceutil/trace.go:171","msg":"trace[38529945] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1435; }","duration":"356.454415ms","start":"2023-06-28T05:45:05.436Z","end":"2023-06-28T05:45:05.792Z","steps":["trace[38529945] 'agreement among raft nodes before linearized reading'  (duration: 356.164255ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:45:05.792Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-06-28T05:45:05.436Z","time spent":"356.614399ms","remote":"127.0.0.1:53532","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":3,"response size":13644,"request content":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" "}
{"level":"warn","ts":"2023-06-28T05:45:06.062Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"126.13331ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:06.063Z","caller":"traceutil/trace.go:171","msg":"trace[342800353] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1436; }","duration":"126.323665ms","start":"2023-06-28T05:45:05.936Z","end":"2023-06-28T05:45:06.063Z","steps":["trace[342800353] 'range keys from in-memory index tree'  (duration: 125.804842ms)"],"step_count":1}
{"level":"info","ts":"2023-06-28T05:45:10.201Z","caller":"traceutil/trace.go:171","msg":"trace[1129621982] linearizableReadLoop","detail":"{readStateIndex:1729; appliedIndex:1729; }","duration":"264.937815ms","start":"2023-06-28T05:45:09.936Z","end":"2023-06-28T05:45:10.201Z","steps":["trace[1129621982] 'read index received'  (duration: 264.928535ms)","trace[1129621982] 'applied index is now lower than readState.Index'  (duration: 7.937¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:10.483Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"150.295716ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/mutatingwebhookconfigurations/\" range_end:\"/registry/mutatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2023-06-28T05:45:10.483Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"391.919403ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-06-28T05:45:10.483Z","caller":"traceutil/trace.go:171","msg":"trace[822652412] range","detail":"{range_begin:/registry/mutatingwebhookconfigurations/; range_end:/registry/mutatingwebhookconfigurations0; response_count:0; response_revision:1438; }","duration":"150.525402ms","start":"2023-06-28T05:45:10.332Z","end":"2023-06-28T05:45:10.483Z","steps":["trace[822652412] 'count revisions from in-memory index tree'  (duration: 150.035171ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:45:10.483Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"546.642472ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:10.483Z","caller":"traceutil/trace.go:171","msg":"trace[687689217] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:1438; }","duration":"392.083858ms","start":"2023-06-28T05:45:10.091Z","end":"2023-06-28T05:45:10.483Z","steps":["trace[687689217] 'agreement among raft nodes before linearized reading'  (duration: 110.496038ms)","trace[687689217] 'count revisions from in-memory index tree'  (duration: 281.39634ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:45:10.483Z","caller":"traceutil/trace.go:171","msg":"trace[1027243627] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1438; }","duration":"546.795576ms","start":"2023-06-28T05:45:09.936Z","end":"2023-06-28T05:45:10.483Z","steps":["trace[1027243627] 'agreement among raft nodes before linearized reading'  (duration: 265.07141ms)","trace[1027243627] 'range keys from in-memory index tree'  (duration: 281.428789ms)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:10.483Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-06-28T05:45:09.936Z","time spent":"546.971827ms","remote":"127.0.0.1:53532","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":3,"response size":13644,"request content":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" "}
{"level":"warn","ts":"2023-06-28T05:45:10.483Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-06-28T05:45:10.091Z","time spent":"392.326002ms","remote":"127.0.0.1:53482","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":2,"response size":31,"request content":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true "}
{"level":"warn","ts":"2023-06-28T05:45:10.888Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"173.51822ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-06-28T05:45:10.888Z","caller":"traceutil/trace.go:171","msg":"trace[1493448439] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1438; }","duration":"173.780711ms","start":"2023-06-28T05:45:10.714Z","end":"2023-06-28T05:45:10.888Z","steps":["trace[1493448439] 'agreement among raft nodes before linearized reading'  (duration: 49.680909ms)","trace[1493448439] 'range keys from in-memory index tree'  (duration: 123.788749ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:45:12.333Z","caller":"traceutil/trace.go:171","msg":"trace[1394746669] linearizableReadLoop","detail":"{readStateIndex:1731; appliedIndex:1731; }","duration":"396.471172ms","start":"2023-06-28T05:45:11.937Z","end":"2023-06-28T05:45:12.333Z","steps":["trace[1394746669] 'read index received'  (duration: 396.441593ms)","trace[1394746669] 'applied index is now lower than readState.Index'  (duration: 25.118¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:12.334Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"396.968459ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:12.334Z","caller":"traceutil/trace.go:171","msg":"trace[1854049445] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1439; }","duration":"397.144791ms","start":"2023-06-28T05:45:11.937Z","end":"2023-06-28T05:45:12.334Z","steps":["trace[1854049445] 'agreement among raft nodes before linearized reading'  (duration: 396.791345ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:45:12.334Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-06-28T05:45:11.937Z","time spent":"397.341771ms","remote":"127.0.0.1:53532","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":3,"response size":13644,"request content":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" "}
{"level":"warn","ts":"2023-06-28T05:45:15.534Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"156.993487ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2023-06-28T05:45:15.534Z","caller":"traceutil/trace.go:171","msg":"trace[2010144222] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:1442; }","duration":"157.079814ms","start":"2023-06-28T05:45:15.377Z","end":"2023-06-28T05:45:15.534Z","steps":["trace[2010144222] 'agreement among raft nodes before linearized reading'  (duration: 40.266443ms)","trace[2010144222] 'range keys from in-memory index tree'  (duration: 116.672254ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:45:16.979Z","caller":"traceutil/trace.go:171","msg":"trace[512986214] linearizableReadLoop","detail":"{readStateIndex:1738; appliedIndex:1738; }","duration":"133.258502ms","start":"2023-06-28T05:45:16.846Z","end":"2023-06-28T05:45:16.979Z","steps":["trace[512986214] 'read index received'  (duration: 133.249286ms)","trace[512986214] 'applied index is now lower than readState.Index'  (duration: 7.754¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:16.980Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"134.044742ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw.176cbc498273f9fc\" ","response":"range_response_count:1 size:727"}
{"level":"info","ts":"2023-06-28T05:45:16.980Z","caller":"traceutil/trace.go:171","msg":"trace[269606034] range","detail":"{range_begin:/registry/events/ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw.176cbc498273f9fc; range_end:; response_count:1; response_revision:1444; }","duration":"134.190488ms","start":"2023-06-28T05:45:16.846Z","end":"2023-06-28T05:45:16.980Z","steps":["trace[269606034] 'agreement among raft nodes before linearized reading'  (duration: 133.401619ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:45:22.047Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.460737ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:22.047Z","caller":"traceutil/trace.go:171","msg":"trace[908785474] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1448; }","duration":"111.716449ms","start":"2023-06-28T05:45:21.935Z","end":"2023-06-28T05:45:22.047Z","steps":["trace[908785474] 'agreement among raft nodes before linearized reading'  (duration: 84.134861ms)","trace[908785474] 'range keys from in-memory index tree'  (duration: 27.165306ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:45:25.456Z","caller":"traceutil/trace.go:171","msg":"trace[28289915] transaction","detail":"{read_only:false; response_revision:1451; number_of_response:1; }","duration":"139.468943ms","start":"2023-06-28T05:45:25.317Z","end":"2023-06-28T05:45:25.456Z","steps":["trace[28289915] 'process raft request'  (duration: 46.938114ms)","trace[28289915] 'compare'  (duration: 92.294362ms)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:26.041Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.179772ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:26.041Z","caller":"traceutil/trace.go:171","msg":"trace[626600758] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1452; }","duration":"105.361239ms","start":"2023-06-28T05:45:25.935Z","end":"2023-06-28T05:45:26.041Z","steps":["trace[626600758] 'range keys from in-memory index tree'  (duration: 104.950458ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:45:30.038Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"101.65377ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:30.038Z","caller":"traceutil/trace.go:171","msg":"trace[1493637579] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1454; }","duration":"101.853678ms","start":"2023-06-28T05:45:29.936Z","end":"2023-06-28T05:45:30.038Z","steps":["trace[1493637579] 'agreement among raft nodes before linearized reading'  (duration: 30.160102ms)","trace[1493637579] 'range keys from in-memory index tree'  (duration: 71.346655ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:45:31.943Z","caller":"traceutil/trace.go:171","msg":"trace[1577950373] transaction","detail":"{read_only:false; response_revision:1457; number_of_response:1; }","duration":"135.441871ms","start":"2023-06-28T05:45:31.808Z","end":"2023-06-28T05:45:31.943Z","steps":["trace[1577950373] 'process raft request'  (duration: 135.047485ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:45:35.504Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"126.645198ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022062539164010 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1451 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128022062539164008 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2023-06-28T05:45:35.504Z","caller":"traceutil/trace.go:171","msg":"trace[204808172] transaction","detail":"{read_only:false; response_revision:1461; number_of_response:1; }","duration":"183.981704ms","start":"2023-06-28T05:45:35.320Z","end":"2023-06-28T05:45:35.504Z","steps":["trace[204808172] 'process raft request'  (duration: 56.856664ms)","trace[204808172] 'compare'  (duration: 126.309672ms)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:43.620Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"183.14606ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13532"}
{"level":"info","ts":"2023-06-28T05:45:43.620Z","caller":"traceutil/trace.go:171","msg":"trace[469630233] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1466; }","duration":"183.423631ms","start":"2023-06-28T05:45:43.437Z","end":"2023-06-28T05:45:43.620Z","steps":["trace[469630233] 'agreement among raft nodes before linearized reading'  (duration: 64.92488ms)","trace[469630233] 'range keys from in-memory index tree'  (duration: 118.092262ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:45:45.905Z","caller":"traceutil/trace.go:171","msg":"trace[2080506134] transaction","detail":"{read_only:false; response_revision:1470; number_of_response:1; }","duration":"134.229913ms","start":"2023-06-28T05:45:45.771Z","end":"2023-06-28T05:45:45.905Z","steps":["trace[2080506134] 'process raft request'  (duration: 86.433152ms)","trace[2080506134] 'compare'  (duration: 47.30477ms)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:45:55.539Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.413773ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13620"}
{"level":"info","ts":"2023-06-28T05:45:55.539Z","caller":"traceutil/trace.go:171","msg":"trace[1113207316] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:1478; }","duration":"103.613289ms","start":"2023-06-28T05:45:55.435Z","end":"2023-06-28T05:45:55.539Z","steps":["trace[1113207316] 'range keys from in-memory index tree'  (duration: 102.99019ms)"],"step_count":1}
{"level":"info","ts":"2023-06-28T05:46:05.499Z","caller":"traceutil/trace.go:171","msg":"trace[94782949] linearizableReadLoop","detail":"{readStateIndex:1789; appliedIndex:1789; }","duration":"158.264762ms","start":"2023-06-28T05:46:05.341Z","end":"2023-06-28T05:46:05.499Z","steps":["trace[94782949] 'read index received'  (duration: 158.255549ms)","trace[94782949] 'applied index is now lower than readState.Index'  (duration: 7.877¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:46:05.499Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"158.526926ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2023-06-28T05:46:05.499Z","caller":"traceutil/trace.go:171","msg":"trace[2096954618] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:1485; }","duration":"158.675544ms","start":"2023-06-28T05:46:05.341Z","end":"2023-06-28T05:46:05.499Z","steps":["trace[2096954618] 'agreement among raft nodes before linearized reading'  (duration: 158.417278ms)"],"step_count":1}
{"level":"info","ts":"2023-06-28T05:46:14.868Z","caller":"traceutil/trace.go:171","msg":"trace[416510998] linearizableReadLoop","detail":"{readStateIndex:1796; appliedIndex:1796; }","duration":"154.283656ms","start":"2023-06-28T05:46:14.714Z","end":"2023-06-28T05:46:14.868Z","steps":["trace[416510998] 'read index received'  (duration: 154.262041ms)","trace[416510998] 'applied index is now lower than readState.Index'  (duration: 18.377¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:46:14.897Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"183.402138ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-06-28T05:46:14.897Z","caller":"traceutil/trace.go:171","msg":"trace[422506123] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1491; }","duration":"183.617359ms","start":"2023-06-28T05:46:14.714Z","end":"2023-06-28T05:46:14.897Z","steps":["trace[422506123] 'agreement among raft nodes before linearized reading'  (duration: 154.558161ms)","trace[422506123] 'range keys from in-memory index tree'  (duration: 28.771959ms)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:46:20.722Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"115.833636ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022062539164282 > lease_revoke:<id:70cc8900742f4a2f>","response":"size:29"}
{"level":"warn","ts":"2023-06-28T05:46:20.969Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"142.136732ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-06-28T05:46:20.969Z","caller":"traceutil/trace.go:171","msg":"trace[10493345] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:1495; }","duration":"142.379973ms","start":"2023-06-28T05:46:20.827Z","end":"2023-06-28T05:46:20.969Z","steps":["trace[10493345] 'count revisions from in-memory index tree'  (duration: 141.838177ms)"],"step_count":1}
{"level":"warn","ts":"2023-06-28T05:46:25.499Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"153.81855ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2023-06-28T05:46:25.499Z","caller":"traceutil/trace.go:171","msg":"trace[1435892999] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:1499; }","duration":"154.008794ms","start":"2023-06-28T05:46:25.345Z","end":"2023-06-28T05:46:25.499Z","steps":["trace[1435892999] 'agreement among raft nodes before linearized reading'  (duration: 38.82471ms)","trace[1435892999] 'range keys from in-memory index tree'  (duration: 114.897828ms)"],"step_count":2}
{"level":"info","ts":"2023-06-28T05:46:35.478Z","caller":"traceutil/trace.go:171","msg":"trace[208744242] linearizableReadLoop","detail":"{readStateIndex:1816; appliedIndex:1816; }","duration":"125.499531ms","start":"2023-06-28T05:46:35.352Z","end":"2023-06-28T05:46:35.477Z","steps":["trace[208744242] 'read index received'  (duration: 125.47728ms)","trace[208744242] 'applied index is now lower than readState.Index'  (duration: 17.65¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-06-28T05:46:35.478Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"125.747006ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2023-06-28T05:46:35.478Z","caller":"traceutil/trace.go:171","msg":"trace[1837519932] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:1506; }","duration":"125.916804ms","start":"2023-06-28T05:46:35.352Z","end":"2023-06-28T05:46:35.478Z","steps":["trace[1837519932] 'agreement among raft nodes before linearized reading'  (duration: 125.606182ms)"],"step_count":1}

* 
* ==> kernel <==
*  05:46:45 up 2 days, 20:58,  0 users,  load average: 2.95, 2.96, 2.94
Linux minikube 5.19.0-38-generic #39~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 17 21:16:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [e4d7d3011faa] <==
* I0628 05:23:27.201319       1 trace.go:205] Trace[452930]: "GuaranteedUpdate etcd3" audit-id:44dc155e-79b9-4213-91d9-4d86cd9d41fb,key:/daemonsets/kube-system/kube-proxy,type:*apps.DaemonSet (28-Jun-2023 05:23:26.696) (total time: 504ms):
Trace[452930]: ---"Txn call finished" err:<nil> 496ms (05:23:27.200)
Trace[452930]: [504.60101ms] [504.60101ms] END
I0628 05:23:27.201912       1 trace.go:205] Trace[1374832393]: "Update" url:/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy/status,user-agent:kube-controller-manager/v1.25.3 (linux/amd64) kubernetes/434bfd8/system:serviceaccount:kube-system:daemon-set-controller,audit-id:44dc155e-79b9-4213-91d9-4d86cd9d41fb,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Jun-2023 05:23:26.695) (total time: 505ms):
Trace[1374832393]: ---"Write to database call finished" len:2815,err:<nil> 504ms (05:23:27.201)
Trace[1374832393]: [505.813093ms] [505.813093ms] END
I0628 05:29:35.835254       1 trace.go:205] Trace[1283897697]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints (28-Jun-2023 05:29:35.256) (total time: 578ms):
Trace[1283897697]: ---"Txn call finished" err:<nil> 574ms (05:29:35.835)
Trace[1283897697]: [578.317218ms] [578.317218ms] END
I0628 05:30:20.562241       1 alloc.go:327] "allocated clusterIPs" service="kube-system/registry" clusterIPs=map[IPv4:10.99.75.183]
I0628 05:30:22.162028       1 trace.go:205] Trace[1989356107]: "List(recursive=true) etcd3" audit-id:985f490c-ee5c-410a-b773-86d6f42361b9,key:/pods/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:30:21.661) (total time: 500ms):
Trace[1989356107]: [500.142196ms] [500.142196ms] END
I0628 05:30:22.163956       1 trace.go:205] Trace[301015977]: "List" url:/api/v1/namespaces/kube-system/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:985f490c-ee5c-410a-b773-86d6f42361b9,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:30:21.661) (total time: 502ms):
Trace[301015977]: ---"Listing from storage done" 500ms (05:30:22.162)
Trace[301015977]: [502.170354ms] [502.170354ms] END
I0628 05:30:22.168442       1 trace.go:205] Trace[2077179208]: "Create" url:/api/v1/namespaces/kube-system/serviceaccounts/default/token,user-agent:kubelet/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:fddacbb3-4238-474a-a8d0-c1122f9f245a,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (28-Jun-2023 05:30:21.384) (total time: 783ms):
Trace[2077179208]: ---"Write to database call finished" len:152,err:<nil> 783ms (05:30:22.168)
Trace[2077179208]: [783.88961ms] [783.88961ms] END
I0628 05:30:23.674353       1 trace.go:205] Trace[1431700601]: "List(recursive=true) etcd3" audit-id:b2341805-2172-40f2-b162-4934d7958a64,key:/pods/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:30:23.161) (total time: 512ms):
Trace[1431700601]: [512.974193ms] [512.974193ms] END
I0628 05:30:23.674826       1 trace.go:205] Trace[1573562906]: "List" url:/api/v1/namespaces/kube-system/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:b2341805-2172-40f2-b162-4934d7958a64,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:30:23.161) (total time: 513ms):
Trace[1573562906]: ---"Listing from storage done" 513ms (05:30:23.674)
Trace[1573562906]: [513.499825ms] [513.499825ms] END
I0628 05:31:23.687006       1 trace.go:205] Trace[1882270331]: "List(recursive=true) etcd3" audit-id:d536d6c0-19db-4bb7-8b5f-9aeefeca35c5,key:/pods/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:31:23.161) (total time: 525ms):
Trace[1882270331]: [525.753453ms] [525.753453ms] END
I0628 05:31:23.688563       1 trace.go:205] Trace[797609062]: "List" url:/api/v1/namespaces/kube-system/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:d536d6c0-19db-4bb7-8b5f-9aeefeca35c5,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:31:23.161) (total time: 527ms):
Trace[797609062]: ---"Listing from storage done" 525ms (05:31:23.687)
Trace[797609062]: [527.403218ms] [527.403218ms] END
I0628 05:31:25.364436       1 trace.go:205] Trace[1127935664]: "List(recursive=true) etcd3" audit-id:20309065-4564-4231-972b-e73ca6be0fdc,key:/pods/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:31:24.661) (total time: 702ms):
Trace[1127935664]: [702.919599ms] [702.919599ms] END
I0628 05:31:25.365314       1 trace.go:205] Trace[1920706433]: "List" url:/api/v1/namespaces/kube-system/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:20309065-4564-4231-972b-e73ca6be0fdc,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:31:24.661) (total time: 703ms):
Trace[1920706433]: ---"Listing from storage done" 703ms (05:31:25.364)
Trace[1920706433]: [703.914903ms] [703.914903ms] END
I0628 05:31:27.066192       1 trace.go:205] Trace[1710107751]: "List(recursive=true) etcd3" audit-id:cbde0f1e-df40-4c7b-94f6-3316b0929bba,key:/pods/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:31:25.661) (total time: 1404ms):
Trace[1710107751]: [1.404773535s] [1.404773535s] END
I0628 05:31:27.067482       1 trace.go:205] Trace[59425791]: "List" url:/api/v1/namespaces/kube-system/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:cbde0f1e-df40-4c7b-94f6-3316b0929bba,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:31:25.661) (total time: 1406ms):
Trace[59425791]: ---"Listing from storage done" 1404ms (05:31:27.066)
Trace[59425791]: [1.406132683s] [1.406132683s] END
I0628 05:31:27.687983       1 trace.go:205] Trace[1705721318]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints (28-Jun-2023 05:31:25.366) (total time: 2320ms):
Trace[1705721318]: ---"initial value restored" 1697ms (05:31:27.064)
Trace[1705721318]: ---"Txn call finished" err:<nil> 622ms (05:31:27.687)
Trace[1705721318]: [2.320841442s] [2.320841442s] END
I0628 05:31:27.691646       1 trace.go:205] Trace[663511894]: "List(recursive=true) etcd3" audit-id:d1eed9ff-c43d-4a7c-983c-9a17b928215a,key:/pods/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:31:27.161) (total time: 530ms):
Trace[663511894]: [530.281995ms] [530.281995ms] END
I0628 05:31:27.696410       1 trace.go:205] Trace[2085345791]: "List" url:/api/v1/namespaces/kube-system/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:d1eed9ff-c43d-4a7c-983c-9a17b928215a,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:31:27.161) (total time: 535ms):
Trace[2085345791]: ---"Listing from storage done" 530ms (05:31:27.691)
Trace[2085345791]: [535.118656ms] [535.118656ms] END
I0628 05:39:57.102487       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.108.73.116]
I0628 05:39:57.264616       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.111.11.159]
I0628 05:39:57.571447       1 controller.go:616] quota admission added evaluator for: jobs.batch
I0628 05:44:21.212392       1 trace.go:205] Trace[803523515]: "List(recursive=true) etcd3" audit-id:90975277-e835-49d0-9915-8b69778697b6,key:/pods/ingress-nginx,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:44:20.434) (total time: 777ms):
Trace[803523515]: [777.722561ms] [777.722561ms] END
I0628 05:44:21.216191       1 trace.go:205] Trace[110645717]: "List" url:/api/v1/namespaces/ingress-nginx/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:90975277-e835-49d0-9915-8b69778697b6,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:44:20.434) (total time: 781ms):
Trace[110645717]: ---"Listing from storage done" 777ms (05:44:21.212)
Trace[110645717]: [781.48562ms] [781.48562ms] END
I0628 05:45:10.486448       1 trace.go:205] Trace[1698548462]: "List(recursive=true) etcd3" audit-id:c9a0ad93-c7d1-4276-a796-8085a1761653,key:/pods/ingress-nginx,resourceVersion:,resourceVersionMatch:,limit:0,continue: (28-Jun-2023 05:45:09.935) (total time: 550ms):
Trace[1698548462]: [550.807394ms] [550.807394ms] END
I0628 05:45:10.488306       1 trace.go:205] Trace[855594016]: "List" url:/api/v1/namespaces/ingress-nginx/pods,user-agent:minikube/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:c9a0ad93-c7d1-4276-a796-8085a1761653,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Jun-2023 05:45:09.935) (total time: 552ms):
Trace[855594016]: ---"Listing from storage done" 550ms (05:45:10.486)
Trace[855594016]: [552.778525ms] [552.778525ms] END

* 
* ==> kube-controller-manager [2ffce95599f1] <==
* I0628 05:23:13.314190       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0628 05:23:13.320059       1 shared_informer.go:262] Caches are synced for deployment
I0628 05:23:13.322186       1 shared_informer.go:262] Caches are synced for service account
I0628 05:23:13.323339       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I0628 05:23:13.330371       1 event.go:294] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0628 05:23:13.331344       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0628 05:23:13.335093       1 shared_informer.go:262] Caches are synced for stateful set
I0628 05:23:13.336611       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0628 05:23:13.340677       1 shared_informer.go:262] Caches are synced for endpoint
I0628 05:23:13.342941       1 shared_informer.go:262] Caches are synced for PVC protection
I0628 05:23:13.352142       1 shared_informer.go:262] Caches are synced for cronjob
I0628 05:23:13.352199       1 shared_informer.go:262] Caches are synced for TTL after finished
I0628 05:23:13.357645       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0628 05:23:13.364065       1 shared_informer.go:262] Caches are synced for namespace
I0628 05:23:13.387659       1 shared_informer.go:262] Caches are synced for HPA
I0628 05:23:13.427212       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0628 05:23:13.470246       1 shared_informer.go:262] Caches are synced for disruption
I0628 05:23:13.485309       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I0628 05:23:13.486508       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I0628 05:23:13.488112       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0628 05:23:13.488518       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I0628 05:23:13.510008       1 shared_informer.go:262] Caches are synced for resource quota
I0628 05:23:13.513881       1 shared_informer.go:262] Caches are synced for ReplicationController
I0628 05:23:13.557233       1 shared_informer.go:262] Caches are synced for resource quota
I0628 05:23:13.573854       1 range_allocator.go:367] Set node minikube PodCIDR to [10.244.0.0/24]
I0628 05:23:13.858983       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-ppxfb"
I0628 05:23:13.881264       1 shared_informer.go:262] Caches are synced for garbage collector
I0628 05:23:13.887516       1 shared_informer.go:262] Caches are synced for garbage collector
I0628 05:23:13.887555       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0628 05:23:13.922881       1 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-565d847f94 to 1"
I0628 05:23:14.951771       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-565d847f94-nfvr5"
I0628 05:23:18.270095       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I0628 05:30:20.816643       1 event.go:294] "Event occurred" object="kube-system/registry" fieldPath="" kind="ReplicationController" apiVersion="v1" type="Normal" reason="SuccessfulCreate" message="Created pod: registry-lfx4q"
I0628 05:30:21.102279       1 event.go:294] "Event occurred" object="kube-system/registry-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: registry-proxy-k2bct"
I0628 05:39:57.523463       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5959f988fd to 1"
I0628 05:39:57.799677       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:39:57.824321       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5959f988fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5959f988fd-6dnfw"
I0628 05:39:58.049508       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:39:58.211217       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:39:58.211403       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:39:58.211684       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-ppxd5"
I0628 05:39:58.211719       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-pwnp6"
I0628 05:39:58.282307       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:39:58.282416       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:39:58.282488       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:39:58.282668       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:39:58.550860       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:39:58.809007       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:40:34.326099       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:40:34.498075       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:40:39.463807       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:40:39.692554       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:40:40.468558       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:40:40.593568       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:40:40.594131       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0628 05:40:40.850117       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0628 05:40:40.850808       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:40:40.865951       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0628 05:40:40.866223       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0628 05:40:40.918813       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch

* 
* ==> kube-proxy [7d7fba077037] <==
* I0628 05:23:24.767868       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0628 05:23:24.768095       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0628 05:23:24.768171       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0628 05:23:24.790641       1 server_others.go:206] "Using iptables Proxier"
I0628 05:23:24.790679       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0628 05:23:24.790692       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0628 05:23:24.790711       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0628 05:23:24.790742       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0628 05:23:24.790921       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0628 05:23:24.791309       1 server.go:661] "Version info" version="v1.25.3"
I0628 05:23:24.791331       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0628 05:23:24.792021       1 config.go:226] "Starting endpoint slice config controller"
I0628 05:23:24.792090       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0628 05:23:24.792094       1 config.go:444] "Starting node config controller"
I0628 05:23:24.792106       1 shared_informer.go:255] Waiting for caches to sync for node config
I0628 05:23:24.792049       1 config.go:317] "Starting service config controller"
I0628 05:23:24.792384       1 shared_informer.go:255] Waiting for caches to sync for service config
I0628 05:23:24.892877       1 shared_informer.go:262] Caches are synced for node config
I0628 05:23:24.892921       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0628 05:23:24.892986       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-scheduler [1028df588db3] <==
* E0628 05:22:48.062016       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0628 05:22:48.142382       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0628 05:22:48.142471       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0628 05:22:48.212547       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0628 05:22:48.212654       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0628 05:22:48.245504       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0628 05:22:48.245598       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0628 05:22:48.281902       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0628 05:22:48.281984       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0628 05:22:48.305129       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:48.305220       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0628 05:22:48.325065       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0628 05:22:48.325332       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0628 05:22:48.330502       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:48.330592       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0628 05:22:48.467100       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:48.467180       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0628 05:22:48.471693       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0628 05:22:48.471794       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0628 05:22:48.472166       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0628 05:22:48.472250       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0628 05:22:48.472912       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0628 05:22:48.472972       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0628 05:22:48.571204       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:48.571279       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0628 05:22:48.681409       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0628 05:22:48.681504       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0628 05:22:48.766223       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0628 05:22:48.766339       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0628 05:22:49.777498       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0628 05:22:49.777582       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0628 05:22:49.992480       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0628 05:22:49.992558       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0628 05:22:50.079639       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0628 05:22:50.079715       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0628 05:22:50.445955       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0628 05:22:50.446035       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0628 05:22:50.478588       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0628 05:22:50.478670       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0628 05:22:50.482678       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0628 05:22:50.482750       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0628 05:22:50.598891       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0628 05:22:50.599004       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0628 05:22:51.041795       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0628 05:22:51.041870       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0628 05:22:51.056607       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0628 05:22:51.056671       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0628 05:22:51.120075       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0628 05:22:51.120113       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0628 05:22:51.187095       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:51.187129       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0628 05:22:51.198650       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:51.198680       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0628 05:22:51.230489       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:51.230565       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0628 05:22:51.500682       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0628 05:22:51.500711       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0628 05:22:51.503782       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0628 05:22:51.503810       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
I0628 05:22:55.141192       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Wed 2023-06-28 05:21:48 UTC, end at Wed 2023-06-28 05:46:47 UTC. --
Jun 28 05:23:56 minikube kubelet[2041]: E0628 05:23:56.129980    2041 generic.go:411] "PLEG: Write status" err="rpc error: code = Unknown desc = Error: No such container: 4d1163e9687aae05db6cb9f304449f613a5035125956d0b4b72729942c1c914b" pod="kube-system/storage-provisioner"
Jun 28 05:30:20 minikube kubelet[2041]: I0628 05:30:20.881826    2041 topology_manager.go:205] "Topology Admit Handler"
Jun 28 05:30:20 minikube kubelet[2041]: I0628 05:30:20.977787    2041 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r5r2v\" (UniqueName: \"kubernetes.io/projected/de020955-aa88-4f8c-83c0-25974cf48305-kube-api-access-r5r2v\") pod \"registry-lfx4q\" (UID: \"de020955-aa88-4f8c-83c0-25974cf48305\") " pod="kube-system/registry-lfx4q"
Jun 28 05:30:21 minikube kubelet[2041]: I0628 05:30:21.150922    2041 topology_manager.go:205] "Topology Admit Handler"
Jun 28 05:30:21 minikube kubelet[2041]: I0628 05:30:21.281115    2041 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2q9zk\" (UniqueName: \"kubernetes.io/projected/b3d7f41d-e8e3-4551-b29c-a89b678472a9-kube-api-access-2q9zk\") pod \"registry-proxy-k2bct\" (UID: \"b3d7f41d-e8e3-4551-b29c-a89b678472a9\") " pod="kube-system/registry-proxy-k2bct"
Jun 28 05:30:23 minikube kubelet[2041]: E0628 05:30:23.237318    2041 kuberuntime_manager.go:1006] "PodSandboxStatus of sandbox for pod" err="rpc error: code = Unknown desc = Error: No such container: d22be533aa92db1fdc881a8d6611b71bcfa9c7c7062b4e703c9b601fd23d7e06" podSandboxID="d22be533aa92db1fdc881a8d6611b71bcfa9c7c7062b4e703c9b601fd23d7e06" pod="kube-system/registry-lfx4q"
Jun 28 05:30:23 minikube kubelet[2041]: E0628 05:30:23.237446    2041 generic.go:411] "PLEG: Write status" err="rpc error: code = Unknown desc = Error: No such container: d22be533aa92db1fdc881a8d6611b71bcfa9c7c7062b4e703c9b601fd23d7e06" pod="kube-system/registry-lfx4q"
Jun 28 05:30:29 minikube kubelet[2041]: I0628 05:30:29.751981    2041 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="d22be533aa92db1fdc881a8d6611b71bcfa9c7c7062b4e703c9b601fd23d7e06"
Jun 28 05:30:29 minikube kubelet[2041]: I0628 05:30:29.830937    2041 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="9c19141ad91a624e1033907c2322f752ea41b5bae1e30b734ae86dc7b498c09a"
Jun 28 05:31:28 minikube kubelet[2041]: E0628 05:31:28.875328    2041 remote_runtime.go:599] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: a743c7dcba28542a5e05ef4a120557e962dfe259d8e1b243e9a93e79e1c7efd2" containerID="a743c7dcba28542a5e05ef4a120557e962dfe259d8e1b243e9a93e79e1c7efd2"
Jun 28 05:31:28 minikube kubelet[2041]: E0628 05:31:28.875367    2041 kuberuntime_manager.go:1024] "getPodContainerStatuses for pod failed" err="rpc error: code = Unknown desc = Error: No such container: a743c7dcba28542a5e05ef4a120557e962dfe259d8e1b243e9a93e79e1c7efd2" pod="kube-system/registry-proxy-k2bct"
Jun 28 05:31:28 minikube kubelet[2041]: E0628 05:31:28.875383    2041 generic.go:411] "PLEG: Write status" err="rpc error: code = Unknown desc = Error: No such container: a743c7dcba28542a5e05ef4a120557e962dfe259d8e1b243e9a93e79e1c7efd2" pod="kube-system/registry-proxy-k2bct"
Jun 28 05:31:40 minikube kubelet[2041]: E0628 05:31:40.365712    2041 remote_runtime.go:599] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 28f0564a392ddeaffd895a197260344d4125a4c2c91d89bb46e1f5c44098b8f6" containerID="28f0564a392ddeaffd895a197260344d4125a4c2c91d89bb46e1f5c44098b8f6"
Jun 28 05:31:40 minikube kubelet[2041]: E0628 05:31:40.365859    2041 kuberuntime_manager.go:1024] "getPodContainerStatuses for pod failed" err="rpc error: code = Unknown desc = Error: No such container: 28f0564a392ddeaffd895a197260344d4125a4c2c91d89bb46e1f5c44098b8f6" pod="kube-system/registry-lfx4q"
Jun 28 05:31:40 minikube kubelet[2041]: E0628 05:31:40.365925    2041 generic.go:411] "PLEG: Write status" err="rpc error: code = Unknown desc = Error: No such container: 28f0564a392ddeaffd895a197260344d4125a4c2c91d89bb46e1f5c44098b8f6" pod="kube-system/registry-lfx4q"
Jun 28 05:39:58 minikube kubelet[2041]: I0628 05:39:58.007583    2041 topology_manager.go:205] "Topology Admit Handler"
Jun 28 05:39:58 minikube kubelet[2041]: I0628 05:39:58.090766    2041 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert\") pod \"ingress-nginx-controller-5959f988fd-6dnfw\" (UID: \"73e85d85-849f-4956-9f95-865c88ee9cde\") " pod="ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw"
Jun 28 05:39:58 minikube kubelet[2041]: I0628 05:39:58.090891    2041 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-d89l8\" (UniqueName: \"kubernetes.io/projected/73e85d85-849f-4956-9f95-865c88ee9cde-kube-api-access-d89l8\") pod \"ingress-nginx-controller-5959f988fd-6dnfw\" (UID: \"73e85d85-849f-4956-9f95-865c88ee9cde\") " pod="ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw"
Jun 28 05:39:58 minikube kubelet[2041]: E0628 05:39:58.192708    2041 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 28 05:39:58 minikube kubelet[2041]: E0628 05:39:58.192976    2041 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert podName:73e85d85-849f-4956-9f95-865c88ee9cde nodeName:}" failed. No retries permitted until 2023-06-28 05:39:58.692887445 +0000 UTC m=+1016.409309724 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert") pod "ingress-nginx-controller-5959f988fd-6dnfw" (UID: "73e85d85-849f-4956-9f95-865c88ee9cde") : secret "ingress-nginx-admission" not found
Jun 28 05:39:58 minikube kubelet[2041]: I0628 05:39:58.282017    2041 topology_manager.go:205] "Topology Admit Handler"
Jun 28 05:39:58 minikube kubelet[2041]: I0628 05:39:58.283596    2041 topology_manager.go:205] "Topology Admit Handler"
Jun 28 05:39:58 minikube kubelet[2041]: I0628 05:39:58.393440    2041 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kfrcx\" (UniqueName: \"kubernetes.io/projected/bf05dce4-d83d-4cab-87b8-4be706bca2fc-kube-api-access-kfrcx\") pod \"ingress-nginx-admission-create-pwnp6\" (UID: \"bf05dce4-d83d-4cab-87b8-4be706bca2fc\") " pod="ingress-nginx/ingress-nginx-admission-create-pwnp6"
Jun 28 05:39:58 minikube kubelet[2041]: I0628 05:39:58.393514    2041 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ddt7t\" (UniqueName: \"kubernetes.io/projected/3777ce6d-67e3-421c-a34a-c09bb1705e57-kube-api-access-ddt7t\") pod \"ingress-nginx-admission-patch-ppxd5\" (UID: \"3777ce6d-67e3-421c-a34a-c09bb1705e57\") " pod="ingress-nginx/ingress-nginx-admission-patch-ppxd5"
Jun 28 05:39:58 minikube kubelet[2041]: E0628 05:39:58.696410    2041 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 28 05:39:58 minikube kubelet[2041]: E0628 05:39:58.696623    2041 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert podName:73e85d85-849f-4956-9f95-865c88ee9cde nodeName:}" failed. No retries permitted until 2023-06-28 05:39:59.696561865 +0000 UTC m=+1017.412984129 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert") pod "ingress-nginx-controller-5959f988fd-6dnfw" (UID: "73e85d85-849f-4956-9f95-865c88ee9cde") : secret "ingress-nginx-admission" not found
Jun 28 05:39:59 minikube kubelet[2041]: E0628 05:39:59.705992    2041 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 28 05:39:59 minikube kubelet[2041]: E0628 05:39:59.706201    2041 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert podName:73e85d85-849f-4956-9f95-865c88ee9cde nodeName:}" failed. No retries permitted until 2023-06-28 05:40:01.706147407 +0000 UTC m=+1019.422569671 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert") pod "ingress-nginx-controller-5959f988fd-6dnfw" (UID: "73e85d85-849f-4956-9f95-865c88ee9cde") : secret "ingress-nginx-admission" not found
Jun 28 05:40:01 minikube kubelet[2041]: E0628 05:40:01.724642    2041 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 28 05:40:01 minikube kubelet[2041]: E0628 05:40:01.724859    2041 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert podName:73e85d85-849f-4956-9f95-865c88ee9cde nodeName:}" failed. No retries permitted until 2023-06-28 05:40:05.72480185 +0000 UTC m=+1023.441224114 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert") pod "ingress-nginx-controller-5959f988fd-6dnfw" (UID: "73e85d85-849f-4956-9f95-865c88ee9cde") : secret "ingress-nginx-admission" not found
Jun 28 05:40:04 minikube kubelet[2041]: I0628 05:40:04.794401    2041 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="cdd078f0206d037fe422814f4c4c5884bb218ed95799ef8f33f3215a33d75f6a"
Jun 28 05:40:04 minikube kubelet[2041]: I0628 05:40:04.799246    2041 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="a4dd273c0fa8a8647b677b3e7451fde59992d56130ee6fabc8a9566932c58584"
Jun 28 05:40:05 minikube kubelet[2041]: E0628 05:40:05.759725    2041 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 28 05:40:05 minikube kubelet[2041]: E0628 05:40:05.759940    2041 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert podName:73e85d85-849f-4956-9f95-865c88ee9cde nodeName:}" failed. No retries permitted until 2023-06-28 05:40:13.759896359 +0000 UTC m=+1031.476318664 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert") pod "ingress-nginx-controller-5959f988fd-6dnfw" (UID: "73e85d85-849f-4956-9f95-865c88ee9cde") : secret "ingress-nginx-admission" not found
Jun 28 05:40:13 minikube kubelet[2041]: E0628 05:40:13.824563    2041 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 28 05:40:13 minikube kubelet[2041]: E0628 05:40:13.824756    2041 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert podName:73e85d85-849f-4956-9f95-865c88ee9cde nodeName:}" failed. No retries permitted until 2023-06-28 05:40:29.824701717 +0000 UTC m=+1047.541123982 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert") pod "ingress-nginx-controller-5959f988fd-6dnfw" (UID: "73e85d85-849f-4956-9f95-865c88ee9cde") : secret "ingress-nginx-admission" not found
Jun 28 05:40:29 minikube kubelet[2041]: E0628 05:40:29.257043    2041 remote_runtime.go:599] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 3924db5e19ed282f2c418b53f1e5c68322b60fae6e0e725a4260d234ee1c730c" containerID="3924db5e19ed282f2c418b53f1e5c68322b60fae6e0e725a4260d234ee1c730c"
Jun 28 05:40:29 minikube kubelet[2041]: E0628 05:40:29.257083    2041 kuberuntime_manager.go:1024] "getPodContainerStatuses for pod failed" err="rpc error: code = Unknown desc = Error: No such container: 3924db5e19ed282f2c418b53f1e5c68322b60fae6e0e725a4260d234ee1c730c" pod="ingress-nginx/ingress-nginx-admission-create-pwnp6"
Jun 28 05:40:29 minikube kubelet[2041]: E0628 05:40:29.257101    2041 generic.go:411] "PLEG: Write status" err="rpc error: code = Unknown desc = Error: No such container: 3924db5e19ed282f2c418b53f1e5c68322b60fae6e0e725a4260d234ee1c730c" pod="ingress-nginx/ingress-nginx-admission-create-pwnp6"
Jun 28 05:40:29 minikube kubelet[2041]: E0628 05:40:29.860836    2041 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 28 05:40:29 minikube kubelet[2041]: E0628 05:40:29.861044    2041 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert podName:73e85d85-849f-4956-9f95-865c88ee9cde nodeName:}" failed. No retries permitted until 2023-06-28 05:41:01.860986762 +0000 UTC m=+1079.577409026 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/73e85d85-849f-4956-9f95-865c88ee9cde-webhook-cert") pod "ingress-nginx-controller-5959f988fd-6dnfw" (UID: "73e85d85-849f-4956-9f95-865c88ee9cde") : secret "ingress-nginx-admission" not found
Jun 28 05:40:38 minikube kubelet[2041]: I0628 05:40:38.833648    2041 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-ddt7t\" (UniqueName: \"kubernetes.io/projected/3777ce6d-67e3-421c-a34a-c09bb1705e57-kube-api-access-ddt7t\") pod \"3777ce6d-67e3-421c-a34a-c09bb1705e57\" (UID: \"3777ce6d-67e3-421c-a34a-c09bb1705e57\") "
Jun 28 05:40:38 minikube kubelet[2041]: I0628 05:40:38.837560    2041 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3777ce6d-67e3-421c-a34a-c09bb1705e57-kube-api-access-ddt7t" (OuterVolumeSpecName: "kube-api-access-ddt7t") pod "3777ce6d-67e3-421c-a34a-c09bb1705e57" (UID: "3777ce6d-67e3-421c-a34a-c09bb1705e57"). InnerVolumeSpecName "kube-api-access-ddt7t". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 28 05:40:38 minikube kubelet[2041]: I0628 05:40:38.935145    2041 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kfrcx\" (UniqueName: \"kubernetes.io/projected/bf05dce4-d83d-4cab-87b8-4be706bca2fc-kube-api-access-kfrcx\") pod \"bf05dce4-d83d-4cab-87b8-4be706bca2fc\" (UID: \"bf05dce4-d83d-4cab-87b8-4be706bca2fc\") "
Jun 28 05:40:38 minikube kubelet[2041]: I0628 05:40:38.935320    2041 reconciler.go:399] "Volume detached for volume \"kube-api-access-ddt7t\" (UniqueName: \"kubernetes.io/projected/3777ce6d-67e3-421c-a34a-c09bb1705e57-kube-api-access-ddt7t\") on node \"minikube\" DevicePath \"\""
Jun 28 05:40:38 minikube kubelet[2041]: I0628 05:40:38.938537    2041 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/bf05dce4-d83d-4cab-87b8-4be706bca2fc-kube-api-access-kfrcx" (OuterVolumeSpecName: "kube-api-access-kfrcx") pod "bf05dce4-d83d-4cab-87b8-4be706bca2fc" (UID: "bf05dce4-d83d-4cab-87b8-4be706bca2fc"). InnerVolumeSpecName "kube-api-access-kfrcx". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 28 05:40:39 minikube kubelet[2041]: I0628 05:40:39.036571    2041 reconciler.go:399] "Volume detached for volume \"kube-api-access-kfrcx\" (UniqueName: \"kubernetes.io/projected/bf05dce4-d83d-4cab-87b8-4be706bca2fc-kube-api-access-kfrcx\") on node \"minikube\" DevicePath \"\""
Jun 28 05:40:39 minikube kubelet[2041]: I0628 05:40:39.429045    2041 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="a4dd273c0fa8a8647b677b3e7451fde59992d56130ee6fabc8a9566932c58584"
Jun 28 05:40:39 minikube kubelet[2041]: I0628 05:40:39.444848    2041 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="cdd078f0206d037fe422814f4c4c5884bb218ed95799ef8f33f3215a33d75f6a"
Jun 28 05:41:05 minikube kubelet[2041]: I0628 05:41:05.258613    2041 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="b2b5a5b2f597d146e95ab840bd0ecea0fa4d9f99b7597d21b0bae6ac80d10d57"
Jun 28 05:43:04 minikube kubelet[2041]: E0628 05:43:04.263911    2041 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Jun 28 05:43:04 minikube kubelet[2041]: E0628 05:43:04.264094    2041 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Jun 28 05:43:04 minikube kubelet[2041]: E0628 05:43:04.265839    2041 kuberuntime_manager.go:862] container &Container{Name:controller,Image:k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8,Command:[],Args:[/nginx-ingress-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --watch-ingress-without-class=true --configmap=$(POD_NAMESPACE)/ingress-nginx-controller --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services --udp-services-configmap=$(POD_NAMESPACE)/udp-services --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:80,ContainerPort:80,Protocol:TCP,HostIP:,},ContainerPort{Name:https,HostPort:443,ContainerPort:443,Protocol:TCP,HostIP:,},ContainerPort{Name:webhook,HostPort:0,ContainerPort:8443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:LD_PRELOAD,Value:/usr/local/lib/libmimalloc.so,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{94371840 0} {<nil>} 90Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:webhook-cert,ReadOnly:true,MountPath:/usr/local/certificates/,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-d89l8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:&Lifecycle{PostStart:nil,PreStop:&LifecycleHandler{Exec:&ExecAction{Command:[/wait-shutdown],},HTTPGet:nil,TCPSocket:nil,},},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[ALL],},Privileged:nil,SELinuxOptions:nil,RunAsUser:*101,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*true,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod ingress-nginx-controller-5959f988fd-6dnfw_ingress-nginx(73e85d85-849f-4956-9f95-865c88ee9cde): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Jun 28 05:43:04 minikube kubelet[2041]: E0628 05:43:04.265927    2041 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw" podUID=73e85d85-849f-4956-9f95-865c88ee9cde
Jun 28 05:43:04 minikube kubelet[2041]: E0628 05:43:04.779145    2041 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw" podUID=73e85d85-849f-4956-9f95-865c88ee9cde
Jun 28 05:45:16 minikube kubelet[2041]: E0628 05:45:16.767057    2041 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Jun 28 05:45:16 minikube kubelet[2041]: E0628 05:45:16.767192    2041 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Jun 28 05:45:16 minikube kubelet[2041]: E0628 05:45:16.767702    2041 kuberuntime_manager.go:862] container &Container{Name:controller,Image:k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8,Command:[],Args:[/nginx-ingress-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --watch-ingress-without-class=true --configmap=$(POD_NAMESPACE)/ingress-nginx-controller --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services --udp-services-configmap=$(POD_NAMESPACE)/udp-services --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:80,ContainerPort:80,Protocol:TCP,HostIP:,},ContainerPort{Name:https,HostPort:443,ContainerPort:443,Protocol:TCP,HostIP:,},ContainerPort{Name:webhook,HostPort:0,ContainerPort:8443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:LD_PRELOAD,Value:/usr/local/lib/libmimalloc.so,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{94371840 0} {<nil>} 90Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:webhook-cert,ReadOnly:true,MountPath:/usr/local/certificates/,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-d89l8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:&Lifecycle{PostStart:nil,PreStop:&LifecycleHandler{Exec:&ExecAction{Command:[/wait-shutdown],},HTTPGet:nil,TCPSocket:nil,},},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[ALL],},Privileged:nil,SELinuxOptions:nil,RunAsUser:*101,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*true,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod ingress-nginx-controller-5959f988fd-6dnfw_ingress-nginx(73e85d85-849f-4956-9f95-865c88ee9cde): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Jun 28 05:45:16 minikube kubelet[2041]: E0628 05:45:16.767867    2041 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw" podUID=73e85d85-849f-4956-9f95-865c88ee9cde
Jun 28 05:45:31 minikube kubelet[2041]: E0628 05:45:31.767784    2041 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-6dnfw" podUID=73e85d85-849f-4956-9f95-865c88ee9cde

* 
* ==> storage-provisioner [4d1163e9687a] <==
* I0628 05:23:56.797038       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0628 05:23:56.812223       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0628 05:23:56.812268       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0628 05:23:56.819150       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0628 05:23:56.819260       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"5a0188ff-1660-405d-8eaa-47323fe66767", APIVersion:"v1", ResourceVersion:"412", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_0d264f3c-73d4-49d1-b2fd-0ced0708d86d became leader
I0628 05:23:56.819335       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_0d264f3c-73d4-49d1-b2fd-0ced0708d86d!
I0628 05:23:56.920537       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_0d264f3c-73d4-49d1-b2fd-0ced0708d86d!

* 
* ==> storage-provisioner [e49f7a10e3d5] <==
* I0628 05:23:24.176961       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0628 05:23:54.179987       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

